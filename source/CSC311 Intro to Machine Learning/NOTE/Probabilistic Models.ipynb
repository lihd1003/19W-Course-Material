{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood Function\n",
    "The density of the observed data, as a function of parameters $\\theta$. \n",
    "\n",
    "#### Approaches to classification\n",
    "__Discriminative approach__ estimate parameters of decision boundary / class separator directly from labeled examples\n",
    " - How do I separate the classes\n",
    " - learn $p(t|x)$ directly (logistic regression models)\n",
    " - learn mapping from inputs to classes\n",
    " \n",
    "__Generative approach__ model the distribution of inputs characteristic of the class (Bayes classifier)\n",
    " - What does each class \"look\" like?\n",
    " - Build a model of $p(x|t)$\n",
    " - Apply Bayes rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Classifier\n",
    "Given features $x = [x_1,...,x_D]^T$, we want to compute class probabilities using Bayes Rule:\n",
    "$$p(c|x) = \\frac{p(x,c)}{p(x)} = \\frac{p(x|c)p(c)}{p(x)}$$\n",
    "or by text $$\\text{posterior} = \\frac{\\text{class likelihood} \\times {\\text{prior}}}{\\text{Evidence}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Nets\n",
    "We can represent this model using an __directed graphical model__, or __Bayesian network__. \n",
    "\n",
    "This graph structure means the joint distribution factorizes as a product of conditional distribution for each variable given its parent(s). \n",
    "\n",
    "Intuitively, you can think of the edges as reflecting a causal structure. But mathematically, this doesn't hold without additional assumptions. \n",
    "\n",
    "The parameters can be learned efficiently because the log-likelihood decomposes into independent terms for each feature. \n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal l(\\theta) &= \\sum_{i=1}^N \\log p(c^{(i)}, x^{(i)})\\\\\n",
    "&= \\sum_{i=1}^N \\log\\{p(x^{(i)}| c^{(i)})p(c^{(i)})\\}\\\\\n",
    "&= \\sum_{i=1}^N \\log\\{p(c^{(i)}) \\prod_{j=1}^D p(x_j^{(i)}| c^{(i)})\\}\\\\\n",
    "&= \\sum_{i=1}^N \\bigg[\\log p(c^{(i)}) + \\sum_{j=1}^D \\log p(x_j^{(i)}|c^{(i)})\\bigg]\\\\\n",
    "&= \\underset{\\text{Bernoulli log-likelihood of labels}}{\\sum_{i=1}^N \\log p(c^{(i)})} + \\underset{\\text{Bernoulli log-likelihood for feature }x_j}{\\sum_j^D\\sum_i^N \\log p(x_j^{(i)}|c^{(i)})}\n",
    "\\end{align*}\n",
    "\n",
    "Each of these log-likelihood terms depends on different set of parameters, so they can be optimized independently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Inference\n",
    "$$p(c|x)\\propto p(c)\\prod_j^D p(x_j|c)$$\n",
    "For input $x$, predict by comparing the values of $p(c)\\prod_j^D p(x_j|c)$ for different $c$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Parameter Estimation\n",
    "Bayesian approach treats the parameters as random variables. $\\beta$ is the set of parameters in the prior distribution of $\\theta$\n",
    "\n",
    "To define a Bayesian model, we need to specify two distributions:  \n",
    "__prior distribution__$p(\\theta)$, which encodes our beliefs about the parameters __before__ we observe the data.  \n",
    "__likelihood__, same as in MLE\n",
    "\n",
    "When we update our beliefs based on the observations, we compute the __posterior distribution__ using Bayes' rule. \n",
    "$$p(\\theta|\\mathcal D) = \\frac{p(\\theta)p(\\mathcal D|\\theta)}{\\int p(\\theta')p(\\mathcal D|\\theta')d\\theta'}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum A-Posteriori Estimation\n",
    "Find the most likely parameter settings under the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Data\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
