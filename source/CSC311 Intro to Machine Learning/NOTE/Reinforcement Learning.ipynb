{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Problem\n",
    "An agent continually interacts with the environment. How should it choose its actions so that its long-term rewards are maximized. \n",
    "\n",
    "Reinforcement Learning is challenging in \n",
    " - continuous stream of input information, and we have to choose actions\n",
    " - effects of an action depend on the state of the agent in the world \n",
    " - obtain reward that depends on the state and actions \n",
    " - You know only the reward for your action, not other possible actions. \n",
    " - Could be a delay between action and reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov Decision Process (MDP)\n",
    "MDP is the mathematical framework to describe RL problems.\n",
    "\n",
    "A discounted MDP is defined by a tuple $(S, A, P, R, \\gamma)$ where \n",
    " \n",
    " - $S$ state space, discrete or continuous (most cases discrete)\n",
    " - $A$ action space. we only consider finite action apace, i.e. $A = \\{a_1,...,a_M\\}$\n",
    " - $P$ transition probability\n",
    " - $R$ immediate reward distribution\n",
    " - $\\gamma$ discount factor $0\\leq \\gamma \\leq 1$\n",
    " \n",
    "The agent has a __state__ $s\\in S$ in the environment. \n",
    "\n",
    "At every time step $t = 0, 1,...$ the agent is at state $S_t$. \n",
    " - Takes an __action__ $A_t$\n",
    " - Moves into a new state $S_{t+1}$, according to the dynamics of the environment and the selected action, i.e., $S_{t+1}\\sim P(\\cdot|S_t, A_t)$\n",
    " - Receives some __reward__ $R_t \\sim R(\\cdot|S_t, A_t, S_{t+1})$\n",
    " - Alternatively, $R_t \\sim R(\\cdot|S_t, A_t)$ or $R_t\\sim R(\\cdot |S_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action selection mechanism is described by a __policy__ $\\pi$, which is a mapping from states to actions, i.e., $A_t = \\pi(S_t)$ (deterministic policy) or $A_t\\sim \\pi(\\cdot|S_t)$ (stochastic policy). \n",
    "\n",
    "The goal is to find a policy $\\pi$ s.t. __long term rewards__ of the agent is maximized. \n",
    "\n",
    "Different notions of long-term reward: \n",
    " - Cumulative/total reward: $R_0 + R_1 + R_2 + ...$\n",
    " - Discounted (cumulative) reward: $R_0 + \\gamma R_1 + \\gamma^2 R_2 + ...$ \n",
    "  - The discount factor $0\\leq \\gamma \\leq 1$ determines how myopic the agent is. \n",
    "  - When $\\gamma >> 0\\Rightarrow $ more myopic, $\\gamma >> 1\\Rightarrow $ less myopic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transition Probability (or Dynamics)\n",
    "The transition probability describes the changes in the state of the agent when it chooses actions \n",
    "$$P(S_{t+1} = s'|S_t = s, A_t = a)$$\n",
    "This model has __Markov property__; the future depends on the past only through the current state. \n",
    "\n",
    "A __policy__ is the action-selection mechanism of the agent, and describes its behavior. Policy can be deterministic or stochastic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function based Reinforcement Learning\n",
    "The expected future reward, and is used to evaluate the desirability of states.  \n",
    "State-value function $V^\\pi$ for policy $pi$ is a function defined as \n",
    "$$V^\\pi(s):= E_\\pi \\bigg[\\sum_{t\\geq 0}\\gamma^t R_t\\mid S_0 = s], R_t\\sim R(\\cdot |S_t, A_t, S_{t+1})\\bigg]$$\n",
    "describes the expected discounted reward if the agent starts from state $s$ following policy $\\pi$. \n",
    "\n",
    "Action-value function $Q^\\pi$ for policy $\\pi$ is \n",
    "$$Q^\\pi(s,a) := E_\\pi\\bigg[\\sum_{t\\geq 0}\\gamma^t R_t\\mid S_0 = s, A_0 = a\\bigg]$$\n",
    "describes the expected discounted reward if the agent starts from state $s$, takes action $a$, and afterwards follows policy $\\pi$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to find a policy $\\pi$ that maximizes the value function, i.e. the optimal value function \n",
    "$$Q^*(s,a)=\\sup_\\pi Q^\\pi(s,a)$$\n",
    "Given $Q^*$, the optimal policy can be obtained as \n",
    "$$\\pi^*(s) = \\arg\\max_a Q^* (s,a)$$\n",
    "The goal of an RL agent is to find a policy $\\pi$ that is close to optimal, $Q^\\pi \\approx Q^*$"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
