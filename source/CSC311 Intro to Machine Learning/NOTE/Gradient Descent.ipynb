{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "Update the parameters based on the gradient for a single training example\n",
    "$$1. \\text{Choose i uniformly}, 2. \\theta \\leftarrow \\theta - \\alpha \\frac{\\partial \\mathcal L^{(i)}}{\\partial \\theta}$$\n",
    "Therefore, for extremely large dataset, you can see some progress before seeing all the data. \n",
    "\n",
    "Such gradient is a biased estimate of the batch gradient\n",
    "$$E(\\partial_\\theta L^{(i)}) = N^{-1}\\sum_{i=1}^n \\partial_\\theta L^{(i)} = \\partial_\\theta J$$\n",
    "Note that by this expectation, we should do sampling with replacement\n",
    "\n",
    "#### Potential Issues\n",
    " - Dependent on the order \n",
    " - Variance can be high, considering some points goes in one direction, while the others goes another\n",
    " \n",
    "#### mini-batch\n",
    "Compute the gradients on a randomly chosen medium-sized set of training example.  \n",
    "\n",
    "Let $M$ be the size of the mini batch, \n",
    " - $M\\uparrow$: computation\n",
    " - $M\\downarrow$: can't exploit vectorization, high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate\n",
    "The learning rate also influences the __fluctuations__ due to the stochasticity of the gradients.\n",
    "\n",
    "##### Strategy\n",
    "Start large, gradually decay the learning rate to reduce the fluctuations\n",
    "\n",
    "By reducing the learning rate, reducing the fluctuations can appear to make the loss drop suddenly, but can come at the expense of long-run performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-convex optimization\n",
    "Have a chance of escaping from local (but non global) minima. If the step-size is too small, it will likely to fall into the local minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GD with Momentum\n",
    "compute an exponentially weighted average of the gradient, and the use the gradient to update the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm\n",
    "\n",
    "initialize $V= 0$\n",
    "$$V\\leftarrow \\beta V + (1-\\beta)\\frac{\\partial E}{\\partial w}$$\n",
    "$$w \\leftarrow w - \\alpha V$$\n",
    "where $\\alpha$ is the learning rate, $\\beta$ is the momentum. Commonly, $\\beta$ is around 0.9"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
