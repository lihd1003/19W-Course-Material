<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />

<title>Neural Networks - Notes Portal</title>

<link rel="icon" type="image/gif" href="https://lihd1003.github.io/assets/images/logo.png">
<link rel="stylesheet" href="https://lihd1003.github.io/assets/css/vendor.css" />
<link rel="stylesheet" href="https://lihd1003.github.io/assets/css/style.css" />
<link href="https://fonts.googleapis.com/css?family=Roboto&display=swap" rel="stylesheet">
<script src="https://kit.fontawesome.com/98fa07784c.js"></script>



<style type="text/css">
/* Overrides of notebook CSS for static HTML export */

div#notebook {
  overflow: visible;
  border-top: none;
}@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="https://lihd1003.github.io/UofT-Course-Material-Repo/index/custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
	<!-- header -->
   <header class="header-sticky header-dark">
    <div class="container">
      <nav class="navbar navbar-expand-lg navbar-dark">
        <a class="navbar-brand" href="./index.html">
          <img class="navbar-logo navbar-logo-light" src="https://lihd1003.github.io/assets/images/logo.png" alt="Logo">
          <img class="navbar-logo navbar-logo-dark" src="https://lihd1003.github.io/assets/images/logo.png" alt="Logo">
        </a>

        <div class="collapse navbar-collapse" id="navbarSupportedContent">
          <ul class="navbar-nav align-items-center mr-auto">
          </ul>

          <ul class="navbar-nav align-items-center mr-0">
            <li class="nav-item">
              <a href="https://github.com/lihd1003" class="nav-link waves-effect waves-light" target="_blank">
                <i class="icon-github mr-2"></i>GitHub
              </a>
            </li>
            <li class="nav-item">
              <a href="https://github.com/lihd1003/UofT-Course-Material-Repo" class="nav-link waves-effect waves-light" target="_blank">
                <i class="icon-star mr-2"></i>Star the Repo
              </a>
            </li>
          </ul>
        </div>
      </nav>
    </div>
  </header>

<section class="hero hero-with-header text-white" data-top-top="transform: translateY(0px);" data-top-bottom="transform: translateY(250px);">
    <div class="image image-overlay" style="background-image:url(https://lihd1003.github.io/assets/images/black.jpg)"></div>
    <div class="container">
      <div class="row align-items-center">
        <div class="col text-shadow">

          <h1 class="mb-0">Neural Networks</h1>
        </div>
      </div>
    </div>
 </section>
 <section class="bg-white sec" id="project">
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area border">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Inspiration-and-Introduction">Inspiration and Introduction<a class="anchor-link" href="#Inspiration-and-Introduction">&#182;</a></h3><p><strong>Unit</strong> simulates a model neuron by 
$$y = \phi(\vec w^T \vec x + b)$$
$$\text{output} = \text{Activation function}(\text{weights}^T\cdot \text{inputs}) + \text{bias}$$</p>
<p>The, by throwing together lots of processing unit, we can form a neural net.</p>
<h4 id="Structure">Structure<a class="anchor-link" href="#Structure">&#182;</a></h4><p>A NN have 3 types of layers: input, hidden layer(s), output</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Feed-forward-neural-network">Feed-forward neural network<a class="anchor-link" href="#Feed-forward-neural-network">&#182;</a></h3><p>A directed acyclic graph of connected neurons, starts from input layer, and ends with output layer.<br>
Each layer is consisted of many units.</p>
<p><strong>Recurrent neural networks</strong> allows cycles</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Multilayer-Perceptrons">Multilayer Perceptrons<a class="anchor-link" href="#Multilayer-Perceptrons">&#182;</a></h4><p>A multiplayer network consisting of fully connected layers.</p>
<ul>
<li>Each hideen layer $i$ connects $N_{i-1}$ input units to $N_i$ output units. </li>
<li><strong>Fully connected layer</strong> in the simplest case, all input units are connected to all output units.</li>
<li>Note that the inputs and outputs for a layer are distinct from the inputs and outputs to the network.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Activation-Functions">Activation Functions<a class="anchor-link" href="#Activation-Functions">&#182;</a></h4><ul>
<li>identity $y =z$</li>
<li>Rectified Linear Unit (ReLU) $y = \max(0,z)$</li>
<li>Soft ReLU $y = \log(1 + e^z)$</li>
<li>Hard Threshold $y = \mathbb I(z &gt; 0)$</li>
<li>Logistic $y = (1+e^{-z})^{-1}$</li>
<li>Hyperbolic Tangent (tanh) $y = \frac{e^z - e^{-z}}{e^z + e^{-z}}$</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area border">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">srelu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">logit</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span>
<span class="n">tanh</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">srelu</span><span class="p">);</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Soft ReLU&quot;</span><span class="p">);</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">logit</span><span class="p">);</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Logistic&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tanh</span><span class="p">);</span> <span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;tanh&quot;</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Consider the layers,<br>
$$\begin{align*}
h^{(1)} &amp;= f^{(1)}(x) = \phi(X W^{(1)} + b^{(1)}) &amp;\text{base}\\
h^{(i)} &amp;= f^{(i)}(x) = \phi(h^{(i-1)} W^{(i)} + b^{(i)}) &amp;\text{induction}\\
y &amp;= f^{(L)}(h^{(L-1)}) &amp;\text{ending}
\end{align*}$$
Therefore, we can consider the network as a composition of functions 
$$y = f^{(L)}\circ ...\circ f^{(1)}(X)$$</p>
<p>Therefore, neural nets provide modularity: we can implement each layer's computations as a black box.</p>
<p>For the last layer, if the task is regression, then the last activation function is identity function; if the task is binary classification, then is sigmoid function</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Neural-Network-vs.-Linear-regression">Neural Network vs. Linear regression<a class="anchor-link" href="#Neural-Network-vs.-Linear-regression">&#182;</a></h4><p>Suppose a layer's activation was the identity, so is equivalent to an affine transformation of the input, then we call it a <strong>linear layer</strong></p>
<p>Consider a sequence of linear layer, it is equivalent to a single linear layer, i.e. a linear regression
$$y = XW^{(h)}\circ...\circ W^{(1)} \equiv X\tilde W, W^{(i)}\in\text{affine}$$</p>
<h4 id="Universality">Universality<a class="anchor-link" href="#Universality">&#182;</a></h4><p>However, multilayer feed-forward neural nets with nonlinear activation function are <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem"><b>universal function approximators</b></a>, they can approximate any function arbitrarily well.</p>
<h5 id="Problems">Problems<a class="anchor-link" href="#Problems">&#182;</a></h5><p>this theorem does not tell how large the network well be<br>
we need to find appropriate weights<br>
will eventually lead to overfit, since it will fit the training set perfectly</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Back-propagation">Back-propagation<a class="anchor-link" href="#Back-propagation">&#182;</a></h3><p>Given a NN model (with number of layers, activation function for each layer). We then have the <strong>weight space</strong> being one coordinate for each weight/bias of the network, in all the layers. Then, we have to compute the gradient of the cost $\partial \mathcal J / \partial \vec w$, a.k.a. $\partial \mathcal L / \partial \vec w$</p>
<p>Consider the NN 
$$y = f^{(L)}...f^{(1)}(X), f^{(i)} = \phi^{(i)}(h^{(i-1)}W^{(i)} + b)$$
$$\mathcal L(\vec w,b)=\frac{(y-t)^2}{2}$$
By chain rule
$$\frac{\partial \mathcal L}{\partial w^{(1)}} = \frac{\partial \mathcal L}{\partial y}\frac{\partial y}{\partial f^{(i)}}\frac{\partial f^{(i)}}{\partial f^{(i-1)}}...\frac{\partial f^{(1)}}{\partial h^{(1)}}\frac{\partial h^{(1)}}{\partial w^{(1)}}$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Univariate-Example">Univariate Example<a class="anchor-link" href="#Univariate-Example">&#182;</a></h5>$$z=wx+b, y = \sigma(z), \mathcal L = \frac{(y-t)^2}{2}$$$$
Loss\rightarrow\\
\begin{matrix}
x&amp;&amp;&amp;&amp;t\\
 &amp;\searrow&amp;&amp;&amp;&amp;\searrow\\
w&amp;\rightarrow&amp;z&amp;\rightarrow&amp;y&amp;\rightarrow&amp;\mathcal L\\
&amp;\nearrow\\
b
\end{matrix}\\
\leftarrow Derivatives$$<p>Denote $\bar y := \frac{\partial L}{\partial y}$, or the <strong>error signal</strong>. This emphasizes that the error signals are just values out program is computing, rather than a mathematical operation. 
Then, 
$$\begin{align*}
\bar y &amp;=\partial_y\mathcal L  = y - t\\
\bar z &amp;= \bar y \partial_z\sigma(z)\\
\bar w &amp;= \bar z x\\
\bar b &amp;= \bar z
\end{align*}$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Multivariate-Perceptron-example">Multivariate Perceptron example<a class="anchor-link" href="#Multivariate-Perceptron-example">&#182;</a></h5>$$
Loss\rightarrow\\
\begin{matrix}
W^{(1)}&amp;&amp;&amp;&amp;W^{(2)}&amp;&amp;t\\
 &amp;\searrow&amp;&amp;&amp;&amp;\searrow&amp;&amp;\searrow\\
x&amp;\rightarrow&amp;z&amp;\rightarrow&amp;h&amp;\rightarrow&amp;y&amp;\rightarrow&amp;\mathcal L\\
&amp;\nearrow&amp;&amp;&amp;&amp;\nearrow\\
b^{(1)}&amp;&amp;&amp;&amp;b^{(2)}
\end{matrix}\\
\leftarrow Derivatives$$<p><strong>Forward pass</strong>
$$\begin{align*}
\vec z &amp;= XW^{(1)} + b^{(1)}
&amp;z_i&amp;=\sum_{j}w_{ij}^{(1)}x_j + b_i^{(1)}\\
\vec h &amp;= \sigma(z)
&amp;h_i&amp;=\sigma(z_i)\\
\vec y &amp;= hW^{(2)} + b^{(2)}
&amp;y_k &amp;= \sum_{i}w_{ki}^{(2)}h_i + b_k^{(2)}\\
\mathcal L &amp;= \frac{\|\vec t - \vec y\|^2}{2}
&amp;\mathcal L &amp;= \frac{1}{2}\sum_k (y_k-t_k)^2
\end{align*}$$</p>
<p><strong>Backward pass</strong></p>
$$\begin{align*}
\bar{\mathcal L} &amp;= 1&amp;...\\
\bar{y} &amp;= \bar{\mathcal L}(\vec y -\vec t)
&amp;\bar{y_k} &amp;= \bar{\mathcal L}(y_k - t_k)\\
\bar{W^{(2)}} &amp;= h^T\bar y
&amp;\bar{w_{ki}^{(2)}}&amp;=\bar y_k h_i\\
\bar{b^{(2)}} &amp;= \bar y 
&amp;\bar{b_k^{(2)}}&amp;=\bar y_k\\
\bar h &amp;= \bar y (W^{(2)})^T
&amp;\bar h_i&amp;= \sum_k \bar y_k w_{ki}^{(2)}\\
\bar z &amp;= \bar h \cdot \partial_z\sigma(z)
&amp;\bar z_i&amp;=\bar h_i d_{z_i}\sigma(z_i)\\
\bar{W^{(1)}} &amp;= x^T\bar z&amp;...\\
\bar{b^{(1)}} &amp;= \bar z&amp;...
\end{align*}$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Computational-Cost">Computational Cost<a class="anchor-link" href="#Computational-Cost">&#182;</a></h4><p>Forward: one add-multiply operation per weight<br>
Backward: two add-multiply operations per weight $\bar w, \bar h$</p>
<p>Therefore, let the number of layers be $L$, number of units for the $l$th layer be $m_l$, then the computation time $\in O(\sum_{l=1}^{L-1} m_im_{l+1})$, since each unit is fully connected with the next layer, and takes the weights as the number of units in its layer.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Overfitting-Preventions">Overfitting Preventions<a class="anchor-link" href="#Overfitting-Preventions">&#182;</a></h3><h4 id="Reduce-size-of-the-weights">Reduce size of the weights<a class="anchor-link" href="#Reduce-size-of-the-weights">&#182;</a></h4><p>Adding regularizations onto each regression</p>
<p>Prevents unnecessary weights<br>
Helps in improving generalization<br>
Makes a smoother model in which the conditioning is good</p>
<h4 id="Early-Stopping">Early Stopping<a class="anchor-link" href="#Early-Stopping">&#182;</a></h4><p>Starts with small weights and let it grow until the performance on the validation set starts getting worse</p>
<p>Because when the weights are very small, every hidden unit is in its linear range, so a net with a large layer of hidden units is linear, and it has no more capacity than a linear net in which the inputs are directly connected to the outputs.</p>
<p>While when the weights grow, the hidden units starts to use their non-linear ranges so the capacity grows.</p>

</div>
</div>
</div>
    </div>
  </div>
</section>
  <script src="https://lihd1003.github.io/assets/js/vendor.js"></script>
  <script src="https://lihd1003.github.io/assets/js/app.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script>
    $("table").addClass("table table-lined")
  </script>
</body>

 


</html>
