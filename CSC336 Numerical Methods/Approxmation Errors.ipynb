{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $A:=$ approximation, $T:=$ true value\n",
    "\n",
    "## Absolute error and relative error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Absolute error__ $A-T$  \n",
    "__Relative error__ $\\frac{A-T}{T}$, let $A-T = \\delta$, then $A = T(1+\\delta)$ avoids the issue with $A = 0$\n",
    "\n",
    "__Connect Digits__ $A:= 5.46729 \\times 10^{-12}, T:= 5.46417\\times 10^{-12}$  \n",
    "Then, $\\delta_{absolute}= A-T = 0.000312 \\times 10^{-12}$  \n",
    "$\\delta_{relative} = 0.000312 / 5.46417 = 3.12/5.46417 \\times 10^{-3}$  \n",
    "Let $p$ be the first non-agreed digit, then the magnitude relative error will be around $10^{-p\\pm 1}$.\n",
    "\n",
    "__0.99... = 1__ However, consider $A= 1.00596\\times 10^{-10}, T = 0.99452 \\times 10^{-10}$, the relative error is much smaller than the magnitude approximation. \n",
    "Therefore, we say that $0.999...$ agrees with $1.000...$, hence $A,T$ agrees on 2 sig digits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data error and computational error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data error__ let $\\hat x:=$ input value, $x$ be actual value, error = $\\hat x - x$, for example\n",
    " - we cannot represent $A$ as a terminating floating number $(3.14\\sim \\pi)$\n",
    " - measurement error \n",
    " \n",
    "__Computational error__ let $f$ be the actual mapping, $\\hat f$ be the approximation function, errors of the computation is $\\hat f(x) - f(x)$, for example\n",
    " - often $\\cos$ is calculated by its Taylor series, while Taylor series is a infinite sum, and we can only compute by finite approximation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncation error and rounding error (subclasses of computational error)\n",
    "__Truncation error__ The difference between the true result $f$ and the result that would be produced by a given (finite approximation) algorithm using exact arithmetic $\\hat f$. \n",
    "\n",
    "__rounding error__ The difference between the result produced by a given algorithm using exact arithmetic, and the result produced by the same algorithm using a finite-precision, rounded arithmetic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example__ To approximate $f'(x)$ with known $f$  \n",
    "By definition $f'(x) = \\lim_{h\\rightarrow 0} \\frac{f(x+h) - f(x)}{h}$, hence, choose a small but not necessarily 0 value $h$, we can approximate $f'(x)$ for all $x$.   \n",
    "Assuming $f$ is smooth, then \n",
    "\\begin{align}\n",
    "\\frac{f(x+h)-f(x)}{h} &= \\frac{1}{h}(\\sum_{k=0} f^{(k)}(x)\\frac{h^{k}}{k!} -f(x))&\\text{Taylor expansion around }x \\\\ \n",
    "&= \\frac{1}{h}(f(x)+f'(x)h + f''(c)h^2/2 - f(x)) &\\text{Remainder theorem}, c\\in[x, x+h]\\\\\n",
    "&= f'(x) + \\frac{f''(c)h}{2}\n",
    "\\end{align}\n",
    "Therefore, the truncation error will be $\\frac{f''(c)h}{2}$, by IVT, such error is bounded since $f$ is continuous.\n",
    "\n",
    "Then, let $FL$ be the mapping to the floating representation result, assume \n",
    "\\begin{align}\n",
    "FL(f(x)) &= f(x) + \\epsilon_0 \\\\\n",
    "FL(f(x+h)) &= f(x+h) + \\epsilon_h \\\\\n",
    "FL(f'(x)) &= f'(x) + \\hat\\epsilon\\\\\n",
    "\\Rightarrow FL\\bigg(\\frac{f(x+h) - f(x)}{h} -f'(x)\\bigg)&= \\frac{f(x+h) + \\epsilon_h - (f(x) + \\epsilon_0)}{h} -(f'(x) + \\hat\\epsilon) \\\\\n",
    "&= \\frac{f''(c)h}{2} + \\frac{\\epsilon_h - \\epsilon_0}{h}+ \\hat\\epsilon \\\\\n",
    "\\text{Total Error}&=\\text{Truncation Error + Rounding Error}\n",
    "\\end{align}\n",
    "\n",
    "Note that $\\epsilon_h$ is less influenced by $h$ when $h$ is small, then we can looks $\\epsilon_h -\\epsilon_0 =:\\epsilon$ as a constant.  \n",
    "Then, let $E(h)$ be the total error, $E'(h) = f''(c)/2 - \\frac{\\epsilon}{h^2}\\Rightarrow argmin(E(h))=\\sqrt{\\epsilon_h/M}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Claim__ $\\lim_{h\\rightarrow 0} FL(f(x+h)) - FL(f(x)) = 0$\n",
    "\n",
    "Notice that when $h$ is particularly smaller than the machine's precision, i.e. $x>>h$, then $x + h$ will be exactly $x$. \n",
    "\n",
    "Therefore, for very small $h$, $FL\\bigg(\\frac{f(x+h) - f(x)}{h} -f'(x)\\bigg) = FL(-f'(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Error and backward error\n",
    "\n",
    "Let $y = f(x)$ be the true result, $\\hat y = COMP(f(x))$ be the computational result, where $COMP$ is all the possible errors caused in the computation. Then, __forward error__ is $\\hat y - y$.  \n",
    "Take $\\hat x$ such that with the true computation, $\\hat y = f(\\hat x)$, then __backward error__ is $\\hat x - x$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning of Problems\n",
    "\n",
    "Let $\\hat x \\in (x-\\delta, x+\\delta)$, i.e. any $\\hat x$ within the neighborhood of some $x$.  \n",
    "Consider the relative forward error \n",
    "\\begin{align*}\n",
    "\\frac{\\hat y - y}{y} &:= \\frac{f(\\hat x) - f(x)}{f(x)} \\\\\n",
    "&= \\frac{f'(c)}{f(x)}(\\hat x - x) &\\text{MVT, }c\\in [x, \\hat x]\\\\\n",
    "&= \\frac{xf'(c)}{f(x)}\\frac{\\hat x - x}{x} \\\\\n",
    "&= \\text{conditioning number} \\times \\text{relative backward error}\n",
    "\\end{align*}\n",
    "\n",
    "The problem is __well conditioned__ if $\\forall x$, condition number is small; is __ill conditioned__ if $\\exists x$, condition number is large"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
