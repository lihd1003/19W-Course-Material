{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Midterm Review\n",
    "order: 55\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov's Inequality\n",
    "$$P(|Y| > \\epsilon) \\leq \\frac{E[|Y|^r]}{\\epsilon^t}$$\n",
    "## Chebyshev's Inequality\n",
    "If $r=2$ and $Y=X-E(X)$\n",
    "$$P(|X-E(X)|>\\epsilon) \\leq \\frac{var(X)}{\\epsilon^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converges in probability \n",
    "$$\\lim_{n\\rightarrow\\infty} P(|Y_n - Y| > \\epsilon) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weak Law of Large Numbers\n",
    "If $X_1,X_2,...$ are indep. r.v. with finite $\\mu$ then \n",
    "$$\\frac1n \\sum_{i=1}^nX_i\\rightarrow^p\\mu$$\n",
    "(proven in Checbyshev's Inequality as $var(\\bar X_n)\\rightarrow 0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit Theorem\n",
    "finite mean and variance $\\mu, \\sigma^2$. If $S_n = \\sqrt n (\\bar X_n - \\mu)$\n",
    "$$\\lim_{n\\rightarrow\\infty} P(S_n\\leq x) = \\frac1{\\sigma \\sqrt{2\\pi}}\\int_{-\\infty}^x \\exp(-\\frac{t^2}{2\\sigma^2})dt\\sim N(0,\\sigma^2)$$\n",
    "i.e. converge to $N(0,\\sigma^2)$ in distribution\n",
    "\n",
    "## General CLT\n",
    "$X_1,X_2,...$ indep. with $E(X_i) = \\mu_i, var(X_i) = \\sigma^2_i$ then \n",
    "$$S_n = \\sqrt n (\\frac1n\\sum_{i=1}^n X_i - \\frac1n\\sum_{i=1}^n\\mu_i) \\rightarrow^d N(0, \\frac1n\\sum_{i=1}^n \\sigma_i^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slutsky's Theorem\n",
    "For continuous $\\psi$ and $X_n\\rightarrow^d X$ and $Y_n\\rightarrow^p \\theta$  \n",
    "$$\\psi(X_n,Y_n)\\rightarrow^d \\psi(X,\\theta)$$\n",
    "Some useful examples\n",
    "$$X_n + Y_n\\rightarrow^d X+\\theta, X_nY_n\\rightarrow^d \\theta X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Method\n",
    "If $a_n(X_n-\\theta)\\rightarrow^d Z$ with $a_n\\rightarrow \\infty$. If $g(x)$ is differentiable at $x=\\theta$, then \n",
    "$$a_n(g(X_n) - g(\\theta))\\rightarrow^d g'(\\theta)Z$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substitution Principle\n",
    "Given $X_1,...,X_n$ from $F$, to estimate $\\theta(F)$ where $\\theta$ is some statistics of $F$. Then, We first estimate $F\\rightarrow \\hat F$ and substitute $\\hat F$ for $F$ into $\\theta(F)$\n",
    "$$\\hat\\theta(F) = \\theta(\\hat F)$$\n",
    "If $\\theta$ continuous and $\\hat F\\approx F$ then $\\theta(\\hat F) \\approx \\theta(F)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order Statistics\n",
    "Assume $X_1,...,X_n$ independent with cdf $F$\n",
    "## Sample min/max\n",
    "$$P(X_{(1)} \\leq x) = 1-[1-F(x)]^n, P(X_{(n)}\\leq x) = F(x)^n$$\n",
    "Then the densities\n",
    "$$g_1(x) = n[1-F(x)]^{n-1}f(x), g_n(x) = nF(x)^{n-1}f(x)$$\n",
    "\n",
    "## Distribution of $X_{(k)}$\n",
    "Define $Z(x) = \\sum_{i=1}^n \\mathbb I(X_i\\leq x)\\sim Binomial(n,F(x))$, then $X_{(k)}\\leq x \\equiv Z(x)\\geq k$\n",
    "$$P(X_{(k)}\\leq x)=P(Z(x)\\geq k) = \\sum_{i=k}^n {n\\choose i}F(x)^i[1-F(x)]^{n-i}$$\n",
    "and density is \n",
    "$$g_k(x) = \\frac{n!}{(k-1)!(n-k)!}F(x)^{k-1}[1-F(x)]^{n-k}f(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Order Statistics\n",
    "If $\\{k_n\\}$ is a sequence of integers with $\\sqrt{n}(\\frac{k_n}{n}-\\tau)\\rightarrow 0$ for some $\\tau\\in(0,1)$ and $f(F^{-1}(\\tau)) > 0$\n",
    "$$\\sqrt n (X_{(k_n)} - F^{-1}(\\tau))\\rightarrow^d N\\big(0, \\frac{\\tau(1-\\tau)}{f^2(F^{-1}(\\tau))}\\big)$$\n",
    "i.e. \n",
    "$$\\widehat{F^{-1}(\\tau)} = X_{(k)} \\sim N(F^{-1}(\\tau), \\frac{\\tau(1-\\tau)}{nf^2(F^{-1}(\\tau))})$$\n",
    "with $k\\approx \\tau n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential Spacings\n",
    "$X_1,...,X_n$ are indep. Exponential r.v. with \n",
    "$$f(x;\\lambda) = \\lambda \\exp(-\\lambda x), x\\geq 0$$\n",
    "Given the order statistics $X_{(1)}\\leq ... \\leq X_{(n)}$, define \n",
    "\\begin{align*}\n",
    "Y_1 &= nX_{(1)}\\\\\n",
    "Y_2 &= (n-1)(X_{(2)} - X_{(1)}) = (n-1)D_1\\\\\n",
    "Y_3 &= (n-2)(X_{(3)} - X_{(2)}) = (n-2)D_2\\\\\n",
    "...\\\\\n",
    "Y_n &= X_{(n)}-X_{(n-1)} = D_{n-1}\n",
    "\\end{align*}\n",
    "Then, $Y_1,...,Y_n$ are indep. r.v. $\\sim f(x;\\lambda)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hazard functions \n",
    "If $X$ is a positive continuous r.v. then its hazard function is \n",
    "$$h(x) = \\frac{f(x)}{1-F(x)}$$\n",
    "Note that \n",
    "$$h(x) = -\\frac{d}{dx}\\ln(1-F(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms\n",
    "Given data $x_1,...,x_n$ and $B_k = [u_{k-1}, u_k)$ for $k=1,...,m$ then \n",
    "$$hist(x) = \\frac{\\sum_{i=1}^n \\mathbb I(x_i \\in B_k)}{n(\\mu_k - u_{k-1})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel density estimation\n",
    "Given the kernel $w$ and a bandwidth $h$, the kernel density estimator is \n",
    "$$\\hat{f_h}(x) = \\frac{1}{nh}\\sum_{i=1}^n w(\\frac{x-X_i}{h})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean square error \n",
    "$$MSE_\\theta(\\hat\\theta) = E_\\theta[(\\hat\\theta - \\theta)^2] = var_\\theta(\\hat\\theta) + [E_\\theta(\\hat\\theta) - \\theta]^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency\n",
    "A sequence of estimators $\\{\\hat\\theta_n\\}$ is consistent for $\\theta$ if for each $\\epsilon > 0 $ and $\\theta\\in \\Theta$\n",
    "$$\\lim_{n\\rightarrow\\infty}P_\\theta(|\\hat\\theta_n - \\theta| > \\epsilon ) = 0, i.e. \\hat\\theta_n\\rightarrow^p \\theta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Method Estimator\n",
    "If $g$ is differentiable then we can approximate the sampling distribution of $\\hat\\theta$ by a normal distribution, then\n",
    "$$\\hat\\theta = g(\\bar X) \\sim N(g(\\mu) = \\theta, [g'(\\mu)]^2 \\frac{\\sigma^2}{n})$$\n",
    "by Delta method. Then\n",
    "$$\\hat{se}(\\hat\\theta) = \\frac{|g'(\\bar X)|S}{\\sqrt n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jackknife s.e. estimator\n",
    "Given the pseudo-values $\\Phi_1,...,\\Phi_n$, we define the jackknife estimator of $se(\\hat\\theta)$\n",
    "$$\\hat{se}(\\hat\\theta) = \\bigg[\\frac{1}{n(n-1)\\sum_{i=1}^n (\\Phi_i - \\bar\\Phi)^2}\\bigg]^{1/2} = \\bigg[\\frac{1}{n(n-1)}\\sum_{i=1}^n(\\hat\\theta_{-i}-\\hat\\theta_{\\cdot})^2\\bigg]^{1/2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MoM estimation\n",
    "To find a statistic $T(X_1,...,X_n)$ s.t. $E_\\theta[T(X_1,...,X_n)] = h(\\theta)$ where $h^{-1}$ is well-defined.\n",
    "\n",
    "### Exponential Distribution\n",
    "$X_1,...,X_n$ indep. with $f(x;\\lambda) = \\lambda \\exp(-\\lambda x)$ for $x\\geq 0$ where $\\lambda > 0$ is unknown. \n",
    "\n",
    "For $r > 0$, $E_\\lambda(X_i^r) = \\lambda^{-r} \\Gamma(r+1)$ so that \n",
    "$$\\frac1n\\sum_{i=1}^n X_i^r  = \\frac{\\Gamma(r+1)}{\\hat\\lambda^r}$$\n",
    "$$\\hat\\lambda(r) = (\\frac{1}{n\\Gamma(r+1)}\\sum_{i=1}^n X_i^r)^{-1/r}$$\n",
    "\n",
    "### Gamma Distribution\n",
    "$$f(x;\\alpha, \\lambda) = \\frac{\\lambda^a x^{a-1}\\exp(-\\lambda x)}{\\Gamma(\\alpha)}\\mathbb I(x\\geq 0)$$\n",
    "Note that $E(X_i) = a/\\lambda, var(X_i) = a/\\lambda^2$\n",
    "Then, by MoM, set $\\bar X = \\frac{\\hat\\alpha }{\\hat\\lambda}, S^2 = \\frac{\\hat\\alpha}{\\hat\\lambda^2}$ then, solves to be $\\hat \\alpha = \\frac{\\bar X^2}{S^2}, \\hat\\lambda = \\frac{\\bar X}{S^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pivotal method\n",
    "To find a r.v. $g(X_1,...,X_n, \\theta)$ whose distribution is independent of $\\theta$ and any other unknown params. \n",
    "$$P_\\theta[g(X_1,...,X_n, \\theta) \\leq x] = G(x)$$ \n",
    "where $G(x)$ is completely known. Then $g(X_1,...,X_n,\\theta) $ is a __pivot__.\n",
    "\n",
    "Given the pivot, the choose $a, b$ so that \n",
    "$$p = P_\\theta[a\\leq g\\leq b] = G(b)-G(a-)$$\n",
    "Then, use $g$ to find $P_\\theta[l(X_1,...,X_n)\\leq \\theta \\leq u(X_1,...,X_\\theta)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sufficient Statistic\n",
    "A statistic $T = (T_1(x),..., T_m(X))$ is sufficient for $\\theta$ if the conditional distribution of $X$ given $T= t$ depends only on $t$\n",
    "### Neyman Factorization Theorem\n",
    "$T$ is sufficient IFF\n",
    "$$f(x;\\theta) = g(T(x);\\theta)h(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Information \n",
    "$$FI(\\theta) = -\\frac{d^2}{d\\theta^2}\\ln \\mathcal L(\\hat\\theta)$$\n",
    "Then, the estimated s.e. of $\\hat\\theta$ is \n",
    "$$\\hat{se}(\\hat\\theta) = [FI(\\theta)]^{-1/2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE\n",
    "The MLE $\\hat\\theta_n$ maximizes $\\phi_n(\\theta)$, which converges in probability for each $\\theta$ to $\\phi(\\theta)$, which is maximized at $\\theta = \\theta_0$"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
