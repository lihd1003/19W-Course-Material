{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Linear Regression\n",
    "order: 30\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular approach of a question\n",
    "- Choose a __model__ describing the relationships between variables of interest\n",
    "- __loss function__ quantifying how bad is the fit\n",
    "- __regularizer__ saying how much we prefer different candidate explanations\n",
    "- fit the model using an __optimization algorithm__\n",
    "\n",
    "For supervised learning  \n",
    "Given: target $t\\in\\mathcal T$ (response, outcome, output, class)  \n",
    "features $x\\in\\mathcal X$ (inputs, covariates, design)  \n",
    "Objective to learn a function $f:\\mathcal X \\rightarrow \\mathcal T$ .s.t. $t\\approx y = f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "$$y = f(\\vec x) = \\sum_{j} w_j x_j +b$$ where $\\vec x$ is the input, $y$ is __prediction__, $\\vec w$ is __wights__, $b$ is the __bias__  \n",
    "$\\vec w, b$ are the __parameters__ \n",
    "\n",
    "In matrix form  \n",
    "$y = XW$ where $$ X=\n",
    "\\begin{bmatrix}  \n",
    "1&[x^{(1)}]^T\\\\\n",
    "...&...\\\\\n",
    "1&[x^{(D)}]^T\n",
    "\\end{bmatrix}, W = \\begin{bmatrix}b&w_1&w_2&...&w_D\\end{bmatrix}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function (Squared error)\n",
    "$\\mathcal L(y, t) = \\frac{(y-t)^2}{2}$ where $y-t$ is the residual and $\\frac{1}{2}$ is just to make the calculations convenient. \n",
    "\n",
    "\n",
    "Therefore, define __cost function__ to be the average over all training examples  \n",
    "$$\\mathcal J(\\vec w, b) = \\frac{\\sum^N (y^{(i)}- t^{(i)})^2}{2} = \\frac{1}{2} \\sum^N (\\vec w^T \\vec x^{(i)} + b - t^{(i)})^2$$\n",
    "\n",
    "To minimize the loss/cost, calculate $\\partial_{w_j} \\mathcal J := 0, \\forall j \\in \\{0,1,2,.., N\\}, w_0 = b$   \n",
    "The resulted $$\\vec w^{L.S.} = (X^TX)^{-1}X^Tt$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving model: Polynomial curve fitting \n",
    "Consider __feature mapping__ $\\psi(x):\\mathbb R^D\\rightarrow \\mathbb R^M$, for example $x\\in\\mathbb R, \\psi(x) = [1, x, x^2]^T$, so that we get a new $\\vec x'$ and can be used into fit\n",
    "\n",
    "#### Underfit and Overfit\n",
    "Underfit: model is too simple to fit the data  \n",
    "Overfit: too complex so that fit the data perfectly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving model: $L^2$ Regularization\n",
    "A function that quantifies how much we prefer one hypothesis vs. another\n",
    "\n",
    "We encourage the weights to be small by choosing as our regularizer the $L^2$ penalty $\\mathcal R(\\vec w) := \\frac{\\|\\vec w\\|^2_2}{2}$  \n",
    "The regularized cost function makes a trade-off between fit to the data and the norm of the weights  \n",
    "$$\\mathcal J_{reg}(\\vec w) = \\mathcal J(\\vec w) + \\lambda \\mathcal R(\\vec w)$$\n",
    "Hence $\\lambda$ is a hyperparameter that we can tune with a validation set and allows to vary penalty on dimensionality. \n",
    "\n",
    "When measuring the validation rate, we still measure $\\mathcal J(\\vec w)$, but for training we will use $\\mathcal J_{reg}(\\vec w)$ for determining $M$\n",
    "\n",
    "__Probelms__ need to make sure $x_i$'s have approximately the same unit so that $\\mathcal R(\\vec w)$ is not dominated by some feature weights\n",
    "\n",
    "For LS, regularized cost gives \n",
    "$$\\vec w_\\lambda^{Ridge} = arg\\min_\\vec w \\mathcal J_{reg}(\\vec w)= (X^T X+\\lambda I)^{-1}X^T t$$\n",
    "\n",
    "### $L^1$ Regularization  \n",
    "$\\mathcal R_{L^1} = \\sum |w_i|$ encourages weights to be exactly zero, we can design regularizers based on whatever property we'd like to encourage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "- Choose a model and a loss function\n",
    "- Formulate an optimization problem\n",
    "- Solve the minimization problem using either direct solution (set derivative to zero) or gradient descent (move $\\vec w$, start with a guess, slowly changes to minimize cost, when direct solution is unavailable)\n",
    "- __vectorize__ \n",
    "- use __features__ to get a more powerful linear model\n",
    "- improve the generalization by adding a __regularizer__\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
