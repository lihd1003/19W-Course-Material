{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminology\n",
    "__Automatic Differentiation (autodiff)__ a general way of taking a program which computes a value, and automatically constructing a procedure for computing derivatives of that value. \n",
    "\n",
    "__Backpropagation__ the special case of autodiff applied to nn \n",
    "\n",
    "__Autograd__ the name of a particular autodiff library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite Differences\n",
    "Note that by the definition of derivative \n",
    "$$\\partial_{x_i}f(x_1,...,x_N) = \\lim_{h\\rightarrow 0}\\frac{f(x_1,...,x_i+h, ..., x_N)-f(x_1,...,x_i, ..., x_N)}{h}$$ or \n",
    "$$\\partial_{x_i}f(x_1,...,x_N) = \\lim_{h\\rightarrow 0}\\frac{f(x_1,...,x_i+h, ..., x_N)-f(x_1,...,x_i-h, ..., x_N)}{2h}$$\n",
    "\n",
    "Finite differences are expensive, since you need to do a forward pass for each derivative. Also, this may include huge numerical error. \n",
    "\n",
    "However, since it directly comes from definition, it is often used for testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autodiff\n",
    "An autodiff system will convert the program into a sequence of primitive ops which have specified routines for computing derivatives. \n",
    "\n",
    "## Computational Graph\n",
    "The `Node` class represents a node of the computation graph, with attributes\n",
    " - value: the actual value computed on a particular set of inputs\n",
    " - function: the primitive operation defining the node\n",
    " - args and kwargs: the arguments the op was called with\n",
    " - parents: the parent `Node`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector-Jacobian Products\n",
    "The __Jacobian__ is the matrix pf partial derivatives\n",
    "$$J = \\frac{\\partial \\vec y}{\\partial \\vec x} = \n",
    "\\begin{bmatrix}\n",
    "\\partial_{x_1}y_1&\\cdots&\\partial_{x_n}y_1\\\\\n",
    "\\vdots&\\ddots&\\vdots\\\\\n",
    "\\partial_{x_1}y_m&\\cdots&\\partial_{x_n}y_m\n",
    "\\end{bmatrix}$$\n",
    "Then, the backprop equation can be written as a VJP \n",
    "$$\\bar{x_j} = \\sum_i \\bar{y_i}\\frac{\\partial y_i}{\\partial x_j}, \\bar x = \\bar y^T J(\\text{row vector}), \\bar x = J^T\\bar y(\\text{col vector})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each primitive op, we must specify VJPs for each of its arguments. This is a function which takes in the output gradient (i.e. $\\bar y$). the answer ($y$), and the arguments ($x$), and returns the input gradient ($\\bar x$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop as Message Passing\n",
    "Each node receives a bunch of messages from its children, which it aggregates to get its error signal. It then passes messages to its parents. Each of these messages is a VJP.   \n",
    "This formulation provides modularity: each node needs to know how to compute its outgoing messages, i.e., the VJPs corresponding to each of its parents (arguments to the function). The implementation of $z$ doesn't need to know where $\\bar z$ came from. "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:csc413]",
   "language": "python",
   "name": "conda-env-csc413-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
