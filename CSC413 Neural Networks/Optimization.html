<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />

<title>Optimization - Notes Portal</title>

<link rel="icon" type="image/gif" href="https://lihd1003.github.io/assets/images/logo.png">
<link rel="stylesheet" href="https://lihd1003.github.io/assets/css/vendor.css" />
<link rel="stylesheet" href="https://lihd1003.github.io/assets/css/style.css" />
<link href="https://fonts.googleapis.com/css?family=Roboto&display=swap" rel="stylesheet">
<script src="https://kit.fontawesome.com/98fa07784c.js"></script>



<style type="text/css">
/* Overrides of notebook CSS for static HTML export */

div#notebook {
  overflow: visible;
  border-top: none;
}@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="https://lihd1003.github.io/UofT-Course-Material-Repo/index/custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
	<!-- header -->
   <header class="header-sticky header-dark">
    <div class="container">
      <nav class="navbar navbar-expand-lg navbar-dark">
        <a class="navbar-brand" href="./index.html">
          <img class="navbar-logo navbar-logo-light" src="https://lihd1003.github.io/assets/images/logo.png" alt="Logo">
          <img class="navbar-logo navbar-logo-dark" src="https://lihd1003.github.io/assets/images/logo.png" alt="Logo">
        </a>

        <div class="collapse navbar-collapse" id="navbarSupportedContent">
          <ul class="navbar-nav align-items-center mr-auto">
          </ul>

          <ul class="navbar-nav align-items-center mr-0">
            <li class="nav-item">
              <a href="https://github.com/lihd1003" class="nav-link waves-effect waves-light" target="_blank">
                <i class="icon-github mr-2"></i>GitHub
              </a>
            </li>
            <li class="nav-item">
              <a href="https://github.com/lihd1003/UofT-Course-Material-Repo" class="nav-link waves-effect waves-light" target="_blank">
                <i class="icon-star mr-2"></i>Star the Repo
              </a>
            </li>
          </ul>
        </div>
      </nav>
    </div>
  </header>

<section class="hero hero-with-header text-white" data-top-top="transform: translateY(0px);" data-top-bottom="transform: translateY(250px);">
    <div class="image image-overlay" style="background-image:url(https://lihd1003.github.io/assets/images/black.jpg)"></div>
    <div class="container">
      <div class="row align-items-center">
        <div class="col text-shadow">

          <h1 class="mb-0">Optimization</h1>
        </div>
      </div>
    </div>
 </section>
 <section class="bg-white sec" id="project">
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Hessian-Matrix">Hessian Matrix<a class="anchor-link" href="#Hessian-Matrix">&#182;</a></h1>$$H = \nabla^2 J = \begin{bmatrix}
\frac{\partial^2 J}{\partial\theta_1^2}&amp;\cdots&amp;\frac{\partial^2 J}{\partial\theta_1\partial \theta_D}\\
\vdots &amp; \ddots &amp;\vdots\\
\frac{\partial^2 J}{\partial\theta_D\theta_1}&amp;\cdots&amp;\frac{\partial^2 J}{\partial\theta^2_D}
\end{bmatrix}$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Locally, a function can be approximated by second-order Taylor approximation
$$J(\theta) \approx J(\theta_0) + \nabla J(\theta_0)^T (\theta-\theta_0) + \frac{1}{2}(\theta-\theta_0)^T H(\theta_0)(\theta-\theta_0)$$
A critical point is a point where the gradient is zero. i.e. 
$$J(\theta) = J(\theta_0) + \frac{1}{2}(\theta-\theta_0)^T H(\theta_0)(\theta-\theta_0)$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A lot of important features of the optimization landscape can be characterized by the eigenvalues of the Hessian $H$.</p>
<p>Recall that a symmetric matrix $H$ has only real eigenvalues and there is an orthogonal basis of eigenvectors, i.e. a <strong>spectral decomposition</strong> $H = Q\Lambda Q^T$ where $Q$.</p>
<p>Therefore, refer $H$ as the <strong>curvature</strong> of a function.<br>
Suppose you move along a line defined by $\theta + tv$ for some vector $v$.<br>
Then, second-order Taylor approximation: 
$$J(\theta + tv) \approx J(\theta) + t\nabla J(\theta)^Tv + \frac{t^2}{2}v^TH(\theta)v$$
Hence, in a direction where $v^THv &gt; 0$, the cost function curves upwards, i.e. has positive curvature. Where $v^THv &lt; 0$, it has negative curvature.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A matrix $A$ is positive definite if $v^TAv &gt; 0$ for all $v\neq 0$. positive semidefinite if $v^TAv \geq 0$.</p>
<p>Equivalently, a matrix is positive definite IFF all its eigenvalues are positive.</p>
<p>Therefore, for any critical point $\theta_*$, if $\exists H(\theta_*)$ exists and is positive definite, then $\theta_*$ is a local minimum.</p>
<p>If $J$ is smooth, then it is convex IFF its $H$ is positive semidefinite everywhere. Therefore, for univariate cases, $H$ it is the second derivative</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Convexity">Convexity<a class="anchor-link" href="#Convexity">&#182;</a></h1><p>Training a network with hidden units cannot be convex because of <strong>permutation symmetries</strong>. Then, we can re-order the hidden units in a way that preserves the function computed by the network.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Problems-with-NN">Problems with NN<a class="anchor-link" href="#Problems-with-NN">&#182;</a></h1><h2 id="Saddle-Points">Saddle Points<a class="anchor-link" href="#Saddle-Points">&#182;</a></h2><p>A saddle points is a point where $\nabla J(\theta) = 0$ or $H(\theta)$ has some positive and some negative eigenvalues, i.e. some directions with positive curvature and some with negative curvature.</p>
<h4 id="Example">Example<a class="anchor-link" href="#Example">&#182;</a></h4><p>Suppose two hidden units with identical incoming and outcoming weights, then the GD will always be 0.<br>
Therefore, do not initialize all the weights to 0, instead, assigning some random values.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Plateaux">Plateaux<a class="anchor-link" href="#Plateaux">&#182;</a></h2><p>A flat region from $0-1$ loss, hard threshold activations, and LS logistic activations.</p>
<h4 id="Example">Example<a class="anchor-link" href="#Example">&#182;</a></h4><p><strong>saturated unit</strong> when it is in the flat region of its activation function. i.e. large value of $z$ in logistic functions, or negative values in ReLU.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Ill-conditioned-curvature">Ill-conditioned curvature<a class="anchor-link" href="#Ill-conditioned-curvature">&#182;</a></h2><p>Suppose $H$ has some large positive eigenvalues and some eigenvalues close to 0. Then, GD bounces back and forth in high curvature directions and makes slow progress in low curvature directions. However, the actual optimal should follow the "valley".</p>
<h3 id="GD-dynamics">GD dynamics<a class="anchor-link" href="#GD-dynamics">&#182;</a></h3><p>COnsider a convex quadratic objective $J(\theta) = \frac{1}{2}\theta^TA \theta$ where $A$ is PSD.<br>
Then, the GD update gives
\begin{align*}
\theta &amp;\leftarrow \theta - \alpha \nabla J(\theta)\\
&amp;= \theta - \alpha A \theta\\
&amp;= (I - \alpha A)\theta
\end{align*}
Solving the recurrence, 
$$\theta = (I - \alpha A)^k \theta_0$$
We can analyze matrix powers such as $(I-\alpha A)^k \theta_0$ using the spectral decomposition.<br>
Let $A = Q\Lambda Q^T$ be the spectral decomposition of $A$. 
\begin{align*}
(I - \alpha A)^k \theta_0 &amp;= (I - \alpha Q\Lambda Q^T)^k \theta_0\\
&amp;= [Q(I - \alpha\Lambda)Q^T]^k \theta_0\\
&amp;= Q(I-\alpha \Lambda)^k Q^T\theta_0
\end{align*}
Hence, in the $Q$ basis, each coordinate gets multiplied by $(1-\alpha\lambda_i)^k$ where the $\lambda_i$ are the eigenvalues of $A$.</p>
<p>Therefore,</p>
<ul>
<li>$0 &lt; \alpha \lambda_i \leq 1$, decays to $0$ at a rate that  depends on $\alpha\lambda_i$</li>
<li>$1 &lt; \alpha \lambda_i \leq 2$, oscillates</li>
<li>$\alpha\lambda_i &gt; 2$, unstable (diverges)</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Hence, we need to set the l.r. $\alpha &lt; 2/\lambda_{max}$ to prevent instability, where $\lambda_{max}$ is the largest eigenvalue, i.e. max curvature.</p>
<p>Therefore, the rate o progress in another direction
$$\alpha\lambda_i &lt; \frac{2\lambda_i}{\lambda_{max}}$$
The quantity $\lambda_{max}/\lambda_{min}$ is known as the <strong>condition number</strong> of $A$. Larger condition numbers imply slower convergence of GD.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then, it can be easily generalized to a quadratic not centered as zero, since the gradient descent dynamics are invariant to translation. 
$$J(\theta) = \frac{1}{2}\theta^TA\theta + b^T\theta + c$$
Since a smooth cost function is well approximated by a convex quadratic in the vicinity of a local optimum, this analysis is a good description of the behavior of GD near a optimum.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Solution">Solution<a class="anchor-link" href="#Solution">&#182;</a></h4><p>Note that this issue is also common for imbalanced weights. To avoid these problems, center inputs to $N(0,1)$, (similar scale and mean).
$$\tilde x_j = \frac{x_j - \mu_j}{\sigma_j}$$
Hidden units may have non-centered activations, some tricks includes replace logistic units with tanh units.<br>
A recent method called batch normalization explicitly centers each hidden activation.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Solution:-Momentum">Solution: Momentum<a class="anchor-link" href="#Solution:-Momentum">&#182;</a></h3>$$p\leftarrow \mu p - \alpha \frac{\partial J}{\partial \theta}, \theta \leftarrow \theta + p$$<p>where $\alpha$ is the learning rate, $\mu$ is the damping param, $\mu &lt; 1$, otherwise, momentum won't diminish.</p>
<p>Momentum dampens the oscillations. In the low curvature directions, the gradients point in the same direction, allowing the parameters to pick up speed.<br>
If the gradient is constant, the params will reach a terminal velocity of 
$$-\frac{\alpha}{1-\mu}\frac{\partial J}{\partial \theta}$$
Momentum sometimes helps a lot, and almost never hurts</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Solution:-RMSprop">Solution: RMSprop<a class="anchor-link" href="#Solution:-RMSprop">&#182;</a></h3><p>A variant of SGD which rescales each coordinate of the gradient to have norm 1 on average by keeping an exponential moving average $s_j$ on the squared gradients. 
$$s_j \leftarrow (1-\gamma)s_j + \gamma (\frac{\partial J}{\partial \theta_i})^2$$
$$\theta_j \leftarrow \theta_j - \frac{\alpha}{\sqrt{s_j + \epsilon}}\frac{\partial J}{\partial \theta_j}$$
If the eigenvectors of the Hessian are axis-aligned, then RMSprop can correct for the curvature.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Mini-batch-Training">Mini-batch Training<a class="anchor-link" href="#Mini-batch-Training">&#182;</a></h2><p>Each entire pass over the dataset is called an <strong>epoch</strong>.<br>
Stochastic gradients computed on larger mini-batches have smaller variance
$$var(\frac1S \sum^S \frac{\partial \mathcal L^{(i)}}{\partial \theta_j}) = \frac{1}{S^2}var(\sum^S\frac{\partial \mathcal L^{(i)}}{\partial \theta_j}) = \frac1Svar(\partial_{\theta_j}\mathcal L^{(i)})$$</p>
<h3 id="Batch-size">Batch size<a class="anchor-link" href="#Batch-size">&#182;</a></h3><ul>
<li>large batches converge in fewer weight updates because each stochastic gradient is less noisy</li>
<li>small batches performs more weight updates per second because each one requires less computation</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Training-time-and-Parallel-Computations">Training time and Parallel Computations<a class="anchor-link" href="#Training-time-and-Parallel-Computations">&#182;</a></h4><ul>
<li>Small batches: an update with size 10 isn't much more expensive than size 1</li>
<li>One size is large enough to saturate the hardware efficiencies, the cost becomes linear in size. GPUs tend to favor larger batch sizes. </li>
</ul>
<h4 id="Convergence">Convergence<a class="anchor-link" href="#Convergence">&#182;</a></h4><ul>
<li>small batches have large gradient noise, so large benefit from increased batch size</li>
<li>large batches SGD approximates the batch gradient descent update, so no further benefit from variance reduction. </li>
</ul>

</div>
</div>
</div>
    </div>
  </div>
</section>
  <script src="https://lihd1003.github.io/assets/js/vendor.js"></script>
  <script src="https://lihd1003.github.io/assets/js/app.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script>
    $("table").addClass("table table-lined")
  </script>
</body>

 


</html>
