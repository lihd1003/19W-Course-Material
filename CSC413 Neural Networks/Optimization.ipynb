{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessian Matrix\n",
    "\n",
    "$$H = \\nabla^2 J = \\begin{bmatrix}\n",
    "\\frac{\\partial^2 J}{\\partial\\theta_1^2}&\\cdots&\\frac{\\partial^2 J}{\\partial\\theta_1\\partial \\theta_D}\\\\\n",
    "\\vdots & \\ddots &\\vdots\\\\\n",
    "\\frac{\\partial^2 J}{\\partial\\theta_D\\theta_1}&\\cdots&\\frac{\\partial^2 J}{\\partial\\theta^2_D}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locally, a function can be approximated by second-order Taylor approximation\n",
    "$$J(\\theta) \\approx J(\\theta_0) + \\nabla J(\\theta_0)^T (\\theta-\\theta_0) + \\frac{1}{2}(\\theta-\\theta_0)^T H(\\theta_0)(\\theta-\\theta_0)$$\n",
    "A critical point is a point where the gradient is zero. i.e. \n",
    "$$J(\\theta) = J(\\theta_0) + \\frac{1}{2}(\\theta-\\theta_0)^T H(\\theta_0)(\\theta-\\theta_0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of important features of the optimization landscape can be characterized by the eigenvalues of the Hessian $H$. \n",
    "\n",
    "Recall that a symmetric matrix $H$ has only real eigenvalues and there is an orthogonal basis of eigenvectors, i.e. a __spectral decomposition__ $H = Q\\Lambda Q^T$ where $Q$. \n",
    "\n",
    "Therefore, refer $H$ as the __curvature__ of a function.   \n",
    "Suppose you move along a line defined by $\\theta + tv$ for some vector $v$.  \n",
    "Then, second-order Taylor approximation: \n",
    "$$J(\\theta + tv) \\approx J(\\theta) + t\\nabla J(\\theta)^Tv + \\frac{t^2}{2}v^TH(\\theta)v$$\n",
    "Hence, in a direction where $v^THv > 0$, the cost function curves upwards, i.e. has positive curvature. Where $v^THv < 0$, it has negative curvature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix $A$ is positive definite if $v^TAv > 0$ for all $v\\neq 0$. positive semidefinite if $v^TAv \\geq 0$. \n",
    "\n",
    "Equivalently, a matrix is positive definite IFF all its eigenvalues are positive. \n",
    "\n",
    "Therefore, for any critical point $\\theta_*$, if $\\exists H(\\theta_*)$ exists and is positive definite, then $\\theta_*$ is a local minimum. \n",
    "\n",
    "If $J$ is smooth, then it is convex IFF its $H$ is positive semidefinite everywhere. Therefore, for univariate cases, $H$ it is the second derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convexity\n",
    "Training a network with hidden units cannot be convex because of __permutation symmetries__. Then, we can re-order the hidden units in a way that preserves the function computed by the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems with NN\n",
    "## Saddle Points \n",
    "A saddle points is a point where $\\nabla J(\\theta) = 0$ or $H(\\theta)$ has some positive and some negative eigenvalues, i.e. some directions with positive curvature and some with negative curvature. \n",
    "\n",
    "#### Example\n",
    "Suppose two hidden units with identical incoming and outcoming weights, then the GD will always be 0.  \n",
    "Therefore, do not initialize all the weights to 0, instead, assigning some random values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plateaux\n",
    "A flat region from $0-1$ loss, hard threshold activations, and LS logistic activations.\n",
    "\n",
    "#### Example \n",
    "__saturated unit__ when it is in the flat region of its activation function. i.e. large value of $z$ in logistic functions, or negative values in ReLU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ill-conditioned curvature\n",
    "Suppose $H$ has some large positive eigenvalues and some eigenvalues close to 0. Then, GD bounces back and forth in high curvature directions and makes slow progress in low curvature directions. However, the actual optimal should follow the \"valley\". \n",
    "\n",
    "### GD dynamics\n",
    "COnsider a convex quadratic objective $J(\\theta) = \\frac{1}{2}\\theta^TA \\theta$ where $A$ is PSD.   \n",
    "Then, the GD update gives\n",
    "\\begin{align*}\n",
    "\\theta &\\leftarrow \\theta - \\alpha \\nabla J(\\theta)\\\\\n",
    "&= \\theta - \\alpha A \\theta\\\\\n",
    "&= (I - \\alpha A)\\theta\n",
    "\\end{align*}\n",
    "Solving the recurrence, \n",
    "$$\\theta = (I - \\alpha A)^k \\theta_0$$\n",
    "We can analyze matrix powers such as $(I-\\alpha A)^k \\theta_0$ using the spectral decomposition.   \n",
    "Let $A = Q\\Lambda Q^T$ be the spectral decomposition of $A$. \n",
    "\\begin{align*}\n",
    "(I - \\alpha A)^k \\theta_0 &= (I - \\alpha Q\\Lambda Q^T)^k \\theta_0\\\\\n",
    "&= [Q(I - \\alpha\\Lambda)Q^T]^k \\theta_0\\\\\n",
    "&= Q(I-\\alpha \\Lambda)^k Q^T\\theta_0\n",
    "\\end{align*}\n",
    "Hence, in the $Q$ basis, each coordinate gets multiplied by $(1-\\alpha\\lambda_i)^k$ where the $\\lambda_i$ are the eigenvalues of $A$. \n",
    "\n",
    "Therefore, \n",
    "- $0 < \\alpha \\lambda_i \\leq 1$, decays to $0$ at a rate that  depends on $\\alpha\\lambda_i$\n",
    "- $1 < \\alpha \\lambda_i \\leq 2$, oscillates\n",
    "- $\\alpha\\lambda_i > 2$, unstable (diverges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we need to set the l.r. $\\alpha < 2/\\lambda_{max}$ to prevent instability, where $\\lambda_{max}$ is the largest eigenvalue, i.e. max curvature. \n",
    "\n",
    "Therefore, the rate o progress in another direction\n",
    "$$\\alpha\\lambda_i < \\frac{2\\lambda_i}{\\lambda_{max}}$$\n",
    "The quantity $\\lambda_{max}/\\lambda_{min}$ is known as the __condition number__ of $A$. Larger condition numbers imply slower convergence of GD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, it can be easily generalized to a quadratic not centered as zero, since the gradient descent dynamics are invariant to translation. \n",
    "$$J(\\theta) = \\frac{1}{2}\\theta^TA\\theta + b^T\\theta + c$$\n",
    "Since a smooth cost function is well approximated by a convex quadratic in the vicinity of a local optimum, this analysis is a good description of the behavior of GD near a optimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "Note that this issue is also common for imbalanced weights. To avoid these problems, center inputs to $N(0,1)$, (similar scale and mean).\n",
    "$$\\tilde x_j = \\frac{x_j - \\mu_j}{\\sigma_j}$$\n",
    "Hidden units may have non-centered activations, some tricks includes replace logistic units with tanh units.   \n",
    "A recent method called batch normalization explicitly centers each hidden activation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Momentum \n",
    "$$p\\leftarrow \\mu p - \\alpha \\frac{\\partial J}{\\partial \\theta}, \\theta \\leftarrow \\theta + p$$\n",
    "where $\\alpha$ is the learning rate, $\\mu$ is the damping param, $\\mu < 1$, otherwise, momentum won't diminish. \n",
    "\n",
    "Momentum dampens the oscillations. In the low curvature directions, the gradients point in the same direction, allowing the parameters to pick up speed.  \n",
    "If the gradient is constant, the params will reach a terminal velocity of \n",
    "$$-\\frac{\\alpha}{1-\\mu}\\frac{\\partial J}{\\partial \\theta}$$\n",
    "Momentum sometimes helps a lot, and almost never hurts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: RMSprop\n",
    "A variant of SGD which rescales each coordinate of the gradient to have norm 1 on average by keeping an exponential moving average $s_j$ on the squared gradients. \n",
    "$$s_j \\leftarrow (1-\\gamma)s_j + \\gamma (\\frac{\\partial J}{\\partial \\theta_i})^2$$\n",
    "$$\\theta_j \\leftarrow \\theta_j - \\frac{\\alpha}{\\sqrt{s_j + \\epsilon}}\\frac{\\partial J}{\\partial \\theta_j}$$\n",
    "If the eigenvectors of the Hessian are axis-aligned, then RMSprop can correct for the curvature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch Training\n",
    "Each entire pass over the dataset is called an __epoch__.  \n",
    "Stochastic gradients computed on larger mini-batches have smaller variance\n",
    "$$var(\\frac1S \\sum^S \\frac{\\partial \\mathcal L^{(i)}}{\\partial \\theta_j}) = \\frac{1}{S^2}var(\\sum^S\\frac{\\partial \\mathcal L^{(i)}}{\\partial \\theta_j}) = \\frac1Svar(\\partial_{\\theta_j}\\mathcal L^{(i)})$$\n",
    "### Batch size\n",
    " - large batches converge in fewer weight updates because each stochastic gradient is less noisy\n",
    " - small batches performs more weight updates per second because each one requires less computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training time and Parallel Computations\n",
    "- Small batches: an update with size 10 isn't much more expensive than size 1\n",
    "- One size is large enough to saturate the hardware efficiencies, the cost becomes linear in size. GPUs tend to favor larger batch sizes. \n",
    "\n",
    "#### Convergence\n",
    "- small batches have large gradient noise, so large benefit from increased batch size\n",
    "- large batches SGD approximates the batch gradient descent update, so no further benefit from variance reduction. "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:csc413]",
   "language": "python",
   "name": "conda-env-csc413-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
