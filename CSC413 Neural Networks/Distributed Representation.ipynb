{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP \n",
    "## Objectives\n",
    "Be able to infer a likely sentance $s$ given the observed speech signal $a$.  \n",
    "\n",
    "## Generative Approach\n",
    "The generative approach is to build two components:  \n",
    "__Observation model__, represented as $p(a|s)$, which tells us how likely the sentence $s$ is to lead to the acoustic signal $a$.   \n",
    "__Prior__, represented as $p(s)$, which tells us how likely a given sentence $s$ is. E.g., it should know that \"recognize speech\" is more likely that \"wreck a nice beach\". \n",
    "\n",
    "Given these components, we can use Bayes' Rule to infer a posterior distribution over sentences given the speech signal: \n",
    "$$p(s|a) = \\frac{p(s)p(a|s)}{\\sum_{s'}p(s')p(a|s')}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modeling\n",
    "\n",
    "Assume having a corpus of sentences $s^{(1)}, ..., s^{(N)}$. The ML criterion says we want our model to maximize the probability our model assigns to the observed sentences. Make the assumption that sentences are independent, so that the objective is $\\max \\prod^N p(s^{(i)})$.\n",
    "\n",
    "Then, the __log probability__ is something we can work with more easily. It also conveniently decomposes as a sum, which is equivalent to cross-entropy loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By chain rule of conditional probability (without any assumptions), \n",
    "$$p(s) = p(w_1,...,w_T) = \\prod^Tp(w_i|w_1,...,w_{i-1})$$\n",
    "With __Markov assumption__ (memoryless model), \n",
    "$$p(w_t|w_1,...,w_{t-1}) = p(w_t\\mid w_{t-3}, w_{t-2}, w_{t-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram\n",
    "Using a conditional probability table, consider the empirical distribution\n",
    "$$p(w_3 | w_1, w_2) = \\frac{p(w_3, w_2, w_1)}{p(w_1, w_2)}\\approx \\frac{\\text{count of phrase w1 w2 w3}}{\\text{count of phrase w1 w2}}$$\n",
    "The above example is $3$-gram\n",
    "\n",
    "#### Problems \n",
    "The number of entries in the conditional probability table is exponential in the context length.  \n",
    "__Data sparsity__: most n-grams never appear in the corpus, even if they are possible (we can use a short context, or smooth the probabilities by adding imaginary counts to solve the problem).  \n",
    "Also, using an ensemble of n-gram models with different $n$ can deal with some data sparsity problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Representations\n",
    "n-gram only have local information of the representations, but words can be related far away, e.g., similar part of sentences, similar meaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Language Model\n",
    "__Input__ previous $K$ words  \n",
    "__Target__ next word  \n",
    "__Loss__ cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bengio's Neural Language Model\n",
    "\n",
    "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "![](./assets/bongio.png)\n",
    "\n",
    "\n",
    "Each word it trained to a distributed representation, which the representation is shared-weights across all models, then we have hidden layers that is to predict the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Vector Embeddings (GloVe)\n",
    "a simpler and faster approach based on a matrix factorization similar to PCA. \n",
    "\n",
    "__Hypothesis__ words with similar distributions have similar meanings  \n",
    "\n",
    "Consider a co-occurrence matrix $X$, which counts the number of times two words appear nearby (eg. distance = 5). This gives $V\\times V$ matrix, $|V|=$vocabulary size\n",
    "\n",
    "__Intuition pump__ we want a rank-K approximation $X\\approx R\\tilde R^T$ where $R$ and $\\tilde R$ are $V\\times K$ matrices. \n",
    "- Each row $r_i$ of $R$ is the $K$-dimensional representation of a word \n",
    "- Each entry is approximated as $x_{ij}\\approx r_i^T \\tilde r_j$\n",
    "- Hence, more similar words are more likely to co-occur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems\n",
    "- $X$ is extremely large, so fitting the above factorization using LS is infeasible.  we can reweight the entries so that only nonzero counts matter\n",
    "- Words counts are heavy-trailed, so we approximate $log x_{ij}$ instead of $x_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final cost function is \n",
    "$$J(R) = \\sum_{i,j}f(x_{ij})(r_i^T\\tilde r_j + b_i + \\tilde b_j - \\log x_{ij})^2$$\n",
    "$$f(x_{ij}) = \\begin{cases}(\\frac{x_{ij}}{100})^{3/4} &x_{ij} < 100 \\\\\n",
    "1 &x_{ij}\\geq 100\\end{cases}$$"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:csc413]",
   "language": "python",
   "name": "conda-env-csc413-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
