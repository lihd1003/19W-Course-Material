<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />

<title>Bias-Variane Decomposition - Notes Portal</title>

<link rel="icon" type="image/gif" href="https://lihd1003.github.io/assets/images/logo.png">
<link rel="stylesheet" href="https://lihd1003.github.io/assets/css/vendor.css" />
<link rel="stylesheet" href="https://lihd1003.github.io/assets/css/style.css" />
<link href="https://fonts.googleapis.com/css?family=Roboto&display=swap" rel="stylesheet">
<script src="https://kit.fontawesome.com/98fa07784c.js"></script>



<style type="text/css">
/* Overrides of notebook CSS for static HTML export */

div#notebook {
  overflow: visible;
  border-top: none;
}@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="https://lihd1003.github.io/UofT-Course-Material-Repo/index/custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
	<!-- header -->
   <header class="header-sticky header-dark">
    <div class="container">
      <nav class="navbar navbar-expand-lg navbar-dark">
        <a class="navbar-brand" href="./index.html">
          <img class="navbar-logo navbar-logo-light" src="https://lihd1003.github.io/assets/images/logo.png" alt="Logo">
          <img class="navbar-logo navbar-logo-dark" src="https://lihd1003.github.io/assets/images/logo.png" alt="Logo">
        </a>

        <div class="collapse navbar-collapse" id="navbarSupportedContent">
          <ul class="navbar-nav align-items-center mr-auto">
          </ul>

          <ul class="navbar-nav align-items-center mr-0">
            <li class="nav-item">
              <a href="https://github.com/lihd1003" class="nav-link waves-effect waves-light" target="_blank">
                <i class="icon-github mr-2"></i>GitHub
              </a>
            </li>
            <li class="nav-item">
              <a href="https://github.com/lihd1003/UofT-Course-Material-Repo" class="nav-link waves-effect waves-light" target="_blank">
                <i class="icon-star mr-2"></i>Star the Repo
              </a>
            </li>
          </ul>
        </div>
      </nav>
    </div>
  </header>

<section class="hero hero-with-header text-white" data-top-top="transform: translateY(0px);" data-top-bottom="transform: translateY(250px);">
    <div class="image image-overlay" style="background-image:url(https://lihd1003.github.io/assets/images/black.jpg)"></div>
    <div class="container">
      <div class="row align-items-center">
        <div class="col text-shadow">

          <h1 class="mb-0">Bias-Variane Decomposition</h1>
        </div>
      </div>
    </div>
 </section>
 <section class="bg-white sec" id="project">
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area border">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">sklearn</span> <span class="k">as</span> <span class="nn">sk</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Loss-function">Loss function<a class="anchor-link" href="#Loss-function">&#182;</a></h2><p>A loss function $L(y,t)$ defines how bad it is if, for some example $x$, the algorithm predicts $y$, but the target is actually $t$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Bias-Variance-Decomposition">Bias-Variance Decomposition<a class="anchor-link" href="#Bias-Variance-Decomposition">&#182;</a></h2><p>Suppose the training set $\mathcal D$ consists of $N$ pairs sampled IID from a sample generating distribution, i.e. $(x^{(i)}, t^{(i)})\sim p_{sample}$<br>
Let $p_{\mathcal D}$ denote the induced distribution over training set $i.e. \mathcal D\sim p_{\mathcal D}$</p>
<p>Pick a fixed query point $\vec x$, then consider an experiment where we sample lots of (say $m$ times) training datasets IID from $p_{\mathcal D}$</p>
<p>Then, we can produce $h_{k,\mathcal D}, k\in\{1,2,...,m\}$ and we compute each classifier's prediction $h_{k,\mathcal D}(\vec x) = y$ at the chosen query point $\vec x$</p>
<p>Therefore, $y$ is a random variable, i.e. $D\Rightarrow h_{\mathcal D}\Rightarrow h_{\mathcal D}(\vec x)=y, \mathcal D$ is randomly chosen.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Basic-setup">Basic setup<a class="anchor-link" href="#Basic-setup">&#182;</a></h3><p>Assume $t$ is deterministic given $x$<br>
There is a distribution over the loss at $\vec x$, with $E_{\mathcal D\sim p_{\mathcal D}}(L(h_{\mathcal D}(x), t))$<br>
For each query point, the expected loss is different. We are interested in quantifying how well our classifier does over $p_{sample}$ average over training set $E_{\vec x\sim p_{sample}, \mathcal D\sim p_{\mathcal D}}(L(h_{\mathcal D}(\vec x), t))$<br>
Then, we can decompose the expected loss</p>
$$\begin{align}
E_{x,D}[(h_D(x) - t)^2] &amp;= E_{x,D}\bigg[\big(h_D(x) - E_D(h_D(x)\mid x) +E_D(h_D(x)\mid x)-t\big)^2\bigg]\\
&amp;= \quad E_{x,D}\bigg[\big(h_D(x) - E_D(h_D(x)\mid x)\big)^2\bigg] \\
&amp;\quad+2E_{x,D}\bigg[\big(h_D(x) - E_D(h_D(x)\mid x)\big)\big(E_D(h_D(x)\mid x)-t\big)\bigg]\\
&amp;\quad + E_{x,D}\bigg[\big(E_D(h_D(x)\mid x)-t\big)^2\bigg]\\
&amp;= E_x\bigg[E_D\big[\big(h_D(x) - E_D(h_D(x)\mid x)\big)^2 \\
&amp;\qquad\quad +2E_D\bigg[\big(h_D(x) - E_D(h_D(x)\mid x)\big)\big(E_D(h_D(x)\mid x)-t\big)\mid x \bigg] &amp;(*)\\
&amp;\qquad\quad + \big(E_D(h_D(x)\mid x)-t\big)^2\mid x\big]\bigg]\\
&amp;= E_{x,D}\bigg[\big(h_D(x)-E_D[h_D(x)\mid x]\big)^2\bigg] + E_{x}\bigg[\big(E_D[h_D(x)\mid x] -t\big)^2\bigg]\\
&amp;= variance + bias
\end{align}$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$\begin{align}
(*) \quad &amp;= (E_D(h_D(x)\mid x)-t\big)&amp;\text{indep. of }D\\
&amp;\qquad \times 2E_D\bigg[\mid x \bigg] &amp; (**)\\
(**) &amp;= E_{D,x}\big(h_D(x) - E_{D_x}[h_D(x)]\big) = 0\\
(*) &amp;= 0
\end{align}$$<p><strong>Bias</strong> defines on average, how close is the classifier to true target</p>
<p><strong>Variance</strong> defines how widely dispersed are the prediction as we generate new datasets</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Bayes-Optimality">Bayes Optimality<a class="anchor-link" href="#Bayes-Optimality">&#182;</a></h3><p>What if $t$ is not deterministic given $\vec x$, i.e. $p(t\mid \vec x)$.</p>
<p>Since there is a distribution over targets, we measure distance from $y_*(x) = E(t\mid \vec x)$<br>
$$\begin{align}
E[(y-t)^2 \mid \vec x] &amp;= E(y^2\mid x) - 2E(yt\mid x) + E(t^2\mid x)\\
&amp;= y^2 - 2yE(t\mid x) + E(t\mid x)^2 + var(t\mid x)\\
&amp;= y^2 - 2yy_*(x) + y_*(x)^2 + var(t\mid x)\\
&amp;=(y - y_*(\vec x))^2 + var[t\mid \vec x]
\end{align}$$<br>
The first term show that s $y=y_*(\vec x)$ is the minimized value</p>
<p><strong>Bayes error / irreducible error</strong> The second term is the inherent unpredicatability, or noise, of the target.</p>
<p>If $Var[t|x] = 0$, the algorithm is <strong>Bayes optimal</strong>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can then decompose the non-deterministic form</p>
$$\begin{align}
E_{x,D,t|x}\bigg[(h_D(x) - t)^2\bigg] &amp;= E_D\bigg[E_{x,t|x}\big[(h_D(x)-y)^2 \mid D\big]\bigg]\\
&amp;= \quad E_{x,D}\big[(h_D(x) - E_t[t|x])^2\big] \\
&amp;\quad\quad + E_{x,t|x}\big[(E_{t|x}[t|x] - t)^2\big]\\
&amp;= \quad E_{x,D}\big[(h_D(x) - E_t[t|x])^2\big]  + E_x\big[var(t\mid x)\big]
\end{align}$$<p>Hence we decompose out the <strong>Bayes error</strong>, where the first two terms are identical to the deterministic case, and will be decomposed into bias and variance</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Bagging">Bagging<a class="anchor-link" href="#Bagging">&#182;</a></h2><h3 id="Intuition">Intuition<a class="anchor-link" href="#Intuition">&#182;</a></h3><p>Suppose we sample $m$ indep. training set $D_i$ from $p_d$, we could then learn a predictor $h_i := h_{D_i}$ based on each one, then take the average $h = \sum^m h_i /m$</p>
<p>Bias unchanged
$$E_{D_1,...,D_m \sim p_d}h(x) = \frac{1}{m} \sum^m E_{D_i\sim p_{d}}h_i(x) = E_{D\sim p_d} h_D(x)$$</p>
<p>Variance becomes $1/m$ of the original 
$$var_{D_1,...,D_m}(h(x)) = \frac{1}{m^2}var_{D_i}(h_i(x)) = \frac{1}{m}var_D(h_D(x))$$</p>
<p>However, we don't such iid datasets from $p_d$</p>
<p>So we have to take a single dataset $D$ with $n$ examples<br>
Generate $m$ new datasets by sampling $n$ training examples from $D$, with replacement<br>
Average the predictions of models trained on each of these</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Problem-with-independence">Problem with independence<a class="anchor-link" href="#Problem-with-independence">&#182;</a></h4><p>Let correlation be $\rho$, the variance with correlated datasets is 
$$var(h(x)) = \frac{1}{m}(1-\rho)\sigma^2 + \rho\sigma^2$$</p>
<p>Ironically, introduce additional variability reduces correlation between samples</p>
<ul>
<li>invest a diversified portfolio, not just one stock</li>
<li>help to use average over multiple algorithms, or multiple configurations</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Random-Forests">Random Forests<a class="anchor-link" href="#Random-Forests">&#182;</a></h3><h4 id="General-Idea">General Idea<a class="anchor-link" href="#General-Idea">&#182;</a></h4><p>When choose each node of the decision tree, choose a random set of $d$ features, and only consider splits on those features</p>
<p>The main idea is to improve the variance reduction of bagging by reducing the correlation between the trees</p>
<p>One of the example for black-box ML algorithm</p>

</div>
</div>
</div>
    </div>
  </div>
</section>
  <script src="https://lihd1003.github.io/assets/js/vendor.js"></script>
  <script src="https://lihd1003.github.io/assets/js/app.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script>
    $("table").addClass("table table-lined")
  </script>
</body>

 


</html>
