<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />

<title>Dimension Reduction - Notes Portal</title>

<link rel="icon" type="image/gif" href="https://lihd1003.github.io/assets/images/logo.png">
<link rel="stylesheet" href="https://lihd1003.github.io/assets/css/vendor.css" />
<link rel="stylesheet" href="https://lihd1003.github.io/assets/css/style.css" />
<link href="https://fonts.googleapis.com/css?family=Roboto&display=swap" rel="stylesheet">
<script src="https://kit.fontawesome.com/98fa07784c.js"></script>



<style type="text/css">
/* Overrides of notebook CSS for static HTML export */

div#notebook {
  overflow: visible;
  border-top: none;
}@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="https://lihd1003.github.io/UofT-Course-Material-Repo/index/custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
	<!-- header -->
   <header class="header-sticky header-dark">
    <div class="container">
      <nav class="navbar navbar-expand-lg navbar-dark">
        <a class="navbar-brand" href="./index.html">
          <img class="navbar-logo navbar-logo-light" src="https://lihd1003.github.io/assets/images/logo.png" alt="Logo">
          <img class="navbar-logo navbar-logo-dark" src="https://lihd1003.github.io/assets/images/logo.png" alt="Logo">
        </a>

        <div class="collapse navbar-collapse" id="navbarSupportedContent">
          <ul class="navbar-nav align-items-center mr-auto">
          </ul>

          <ul class="navbar-nav align-items-center mr-0">
            <li class="nav-item">
              <a href="https://github.com/lihd1003" class="nav-link waves-effect waves-light" target="_blank">
                <i class="icon-github mr-2"></i>GitHub
              </a>
            </li>
            <li class="nav-item">
              <a href="https://github.com/lihd1003/UofT-Course-Material-Repo" class="nav-link waves-effect waves-light" target="_blank">
                <i class="icon-star mr-2"></i>Star the Repo
              </a>
            </li>
          </ul>
        </div>
      </nav>
    </div>
  </header>

<section class="hero hero-with-header text-white" data-top-top="transform: translateY(0px);" data-top-bottom="transform: translateY(250px);">
    <div class="image image-overlay" style="background-image:url(https://lihd1003.github.io/assets/images/black.jpg)"></div>
    <div class="container">
      <div class="row align-items-center">
        <div class="col text-shadow">

          <h1 class="mb-0">Dimension Reduction</h1>
        </div>
      </div>
    </div>
 </section>
 <section class="bg-white sec" id="project">
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let $\mathcal D = \{x^{(1)},...,x^{(N)}\}\subset \mathbb R^D$, so that $N$ instances will form matrix 
$$X = \begin{bmatrix}[\vec x^{(1)}]^T\\...\\ [\vec x^{(N)}]^T\end{bmatrix}$$
each row will be one observation of $D$ faetures,<br>
Let $x^{(i)}\sim N(\mu, \Sigma)$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Dimension-Reduction">Dimension Reduction<a class="anchor-link" href="#Dimension-Reduction">&#182;</a></h2><p>Loss some information (e.g. spread / $\sigma$) by projecting higher dimensions onto lower ones. IN practice, the important features can be accurately captured in a low dimensional subspace.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Projection-onto-a-subspace">Projection onto a subspace<a class="anchor-link" href="#Projection-onto-a-subspace">&#182;</a></h3><p>Given $\mathcal D, \hat\mu=N^{-1}\sum^N x^{(i)}$ be the sample mean.</p>
<p>Want to find a $K &lt;D $ dimensional subspace $S\subset \mathcal R^D$ s.t. $x^{(n)}-\hat\mu$ is "well represented" by a projection onto $K$-dimensional $S$.</p>
<p>Where <strong>projection</strong> is to find the closest point $\tilde x$ on $S$ s.t. $\|x-\tilde x\|$ is minimized.</p>
<p>In a 2-dimensional problem, we are looking for direction $u_1$ along with the data is well-represented, such as direction of higher variance or the direction of min difference after projection, which turns to be the same.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Euclidean-Projection">Euclidean Projection<a class="anchor-link" href="#Euclidean-Projection">&#182;</a></h4><p>$Proj_S(x):=$ projection of $x$ on $S$.</p>
<p>In 2D case, $S$ is the line along the unit vector $u$ (1-D subspace). $u$ is a basis for $S$.</p>
<p>Since $x^Tu = \|x\|\|u\|\cos\theta =\|x\|\cos\theta$
$$Proj_S(x) = x^Tu\cdot u = \|\tilde x\|u$$</p>
<p>In K-D case, we have $K$ basis $u_1,...,u_K \in \mathcal R^D$. and the projection will be 
$$Proj_S(x) = \sum^K (x^Tu_i) u_i = \sum^K z_i u_i$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Center-data">Center data<a class="anchor-link" href="#Center-data">&#182;</a></h4><p>Centering by subtract the mean $\hat\mu$. i.e. the mean (center) be the origin. We need to center the data since we don't want location of data to influence the calculations.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Representation/code">Representation/code<a class="anchor-link" href="#Representation/code">&#182;</a></h4><p>Combine the two above together, we have 
$$\tilde x = \hat\mu + Proj_S(x-\hat\mu) = \hat\mu + \sum^K z_i \vec u_i$$
where $z_k = \vec u_k^T(x-\hat\mu)$</p>
<p>Define matrix $U_{D\times K} = [\vec u_1, ..., \vec u_K]$, then $$\vec z = U^T(x-\hat\mu), \tilde x = \hat\mu + U\vec z = \hat\mu + UU^T(x-\hat\mu)$$ 
We call $UU^T$ the projector on $S$, $U^TU = I$</p>
<p>Both $x,\tilde x\in \mathbb R^D$ but $\tilde x$ is a linear combination of vectors in a lower dimensional subspace with representation $\vec z \in \mathbb R^K$.</p>
<p>We call $\tilde x$ <strong>reconstruction</strong> of $x$, $\vec z$ be its representation(code).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Learning-a-Subspace">Learning a Subspace<a class="anchor-link" href="#Learning-a-Subspace">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since we will definitely lose partial information by dimension reduction, we want a good $D\times K$ matrix $U$ with orthonormal columns.</p>
<p>To measure how "good" the subspace is, propose two criteria:</p>
<p><strong>Minimize reconstruction error</strong>: find vectors in a subspace that are closest to data points 
$$\arg\min_U \frac 1 N \sum^N \|x^{(i)} - \tilde x^{(i)}\|^2$$
<strong>Maximize variance of reconstructions</strong> find a subspace where data has the most variability
$$\arg\max_U \frac 1 N \sum^N \|\tilde x^{(i)} - \hat\mu\|^2$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Noting that 
\begin{align*}
E(\tilde x) &amp;= E(\hat\mu + UU^T(x-\hat\mu))\\
&amp;= \hat\mu + UU^T(E(x)-\hat\mu)\\
&amp;= \hat\mu + UU^T0\\
&amp;= \hat\mu
\end{align*}
So that we can still use mean of $x$ to calculate variance of the reconstruciton</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Equivalence-of-the-criteria">Equivalence of the criteria<a class="anchor-link" href="#Equivalence-of-the-criteria">&#182;</a></h4><p><strong>lemma1</strong> Norm of centered reconstruction is equal to norm of representation
\begin{align*}
\|\tilde x^{(i)} - \hat\mu\|^2 &amp;= (U\vec z^{(i)})^T(U\vec z^{(i)})\\
&amp;=  (\vec z^{(i)})^T U^TU\vec z^{(i)}\\
&amp;= (\vec z^{(i)})^T\vec z^{(i)}&amp;U^TU = I\\
&amp;= \|z^{(i)}\|
\end{align*}</p>
<p><strong>lemma2</strong> $\tilde x^{(i)}-\hat\mu$ is orthogonal to $\tilde x^{(i)} - x^{(i)}$</p>
\begin{align*}
(\tilde x^{(i)}-\hat\mu)^T(\tilde x^{(i)} - x^{(i)}) &amp;= (\hat\mu+UU^T(x^{(i)}-\hat\mu)-\hat\mu)^T(\hat\mu+UU^T(x^{(i)}-\hat\mu) - x^{(i)})\\
&amp;= (x^{(i)}-\hat\mu)^TUU^T(\hat\mu- x^{(i)}+UU^T(x^{(i)}-\hat\mu) )\\
&amp;= (x^{(i)}-\hat\mu)^TUU^T(\hat\mu - x^{(i)}) + (x^{(i)}-\hat\mu)^TUU^TUU^T(x^{(i)}-\hat\mu))\\
&amp;= (x^{(i)}-\hat\mu)^TUU^T(\hat\mu - x^{(i)}) + (x^{(i)}-\hat\mu)^TUU^T(x^{(i)}-\hat\mu))\\
&amp;= 0
\end{align*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Proposition</strong> The two criteria is equivalent $\frac 1 N \sum^N\|x^{(i)}- \tilde x^{(i)}\|^2 = C - \frac1N\sum^N\|\tilde x^{(i)}-\hat\mu\|^2$</p>
<p>By lemma2, since the two vectors are orthogonal, by Pythagorean Theorem
\begin{align*}
\|\tilde x^{(i)} - \hat\mu\|^2 + \|x^{(i)} - \tilde x^{(i)}\|^2 &amp;= \|x^{(i)}-\hat\mu\|^2\\
\frac1N\sum^N \|\tilde x^{(i)} - \hat\mu\|^2 + \frac1N\sum^N\|x^{(i)} - \tilde x^{(i)}\|^2 &amp;= \frac1N\sum^N\|x^{(i)}-\hat\mu\|^2\\
\text{projected variance} + \text{reconstruction error} &amp;= C
\end{align*}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="PCA">PCA<a class="anchor-link" href="#PCA">&#182;</a></h2><h4 id="Spectral-Decomposition-(Eigendecomposition)">Spectral Decomposition (Eigendecomposition)<a class="anchor-link" href="#Spectral-Decomposition-(Eigendecomposition)">&#182;</a></h4><p>If $A_{n\times n}$ is a symmetric matrix (so that has a full set of eigenvectors). Then, $\exists Q_{n\times n}, \Lambda_{n\times n}$ s.t. $A = Q\Lambda Q^T$ where $Q$ is orthogonal matrix formed by $n$ eigenvectors and $\Lambda$ is diagonal with $\lambda_1,...,\lambda_n$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Using Eigendecomposition on the <strong>empirical convariance matrix</strong> $\hat\Sigma = \frac1N \sum^N (x^{(i)}-\hat\mu)(x^{(i)}-\hat\mu)^T$, the optimal PCA subspace is then spanned by some $K$ eigenvectors of $\hat\Sigma$</p>
<p>These eigencectors are called principal components, analogous to the principal axes of an ellipse.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Deriving-PCA-for-$K=1$">Deriving PCA for $K=1$<a class="anchor-link" href="#Deriving-PCA-for-$K=1$">&#182;</a></h4>\begin{align*}
\frac 1N \sum^N\|\tilde x^{(i)} - \hat\mu\|^2 &amp;= \frac1N\sum^N [z^{(i)}]^2\\
&amp;= \frac1N\sum^N(u^T(x^{(i)}-\hat\mu))^2\\
&amp;=  \frac1N\sum^Nu^T(x^{(i)}-\hat\mu)(x^{(i)}-\hat\mu)^Tu\\
&amp;= u^T\bigg[\frac1N\sum^N(x^{(i)}-\hat\mu)(x^{(i)}-\hat\mu)^T\bigg]u\\
&amp;= u^T\hat\Sigma u= u^TQ\Lambda Q^Tu=a^T\Lambda a= \sum^D\lambda_j a_j^2
\end{align*}<p>For the goal of maximize $\sum^D \lambda_j a_j^2, \vec a = Q^Tu$, noting that $\sum a_j = a^Ta = u^TQQ^Tu = u^Tu = 1$. Therefore, choosing the largest $\lambda_k$, 
$$\sum \lambda_ja_j^2 \leq \sum \lambda_k a_j^2 = \lambda_d\sum a_j^2 = \lambda_k$$
And such maximum can be obtained by setting $a_i = \mathbb I(i = k)$. Therefore, $\vec u = Q\vec a = q_k$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Decorrelation">Decorrelation<a class="anchor-link" href="#Decorrelation">&#182;</a></h4>\begin{align*}
cov(\vec z) &amp;= cov(U^T(x-\mu))\\
&amp;= U^Tcov(x)U\\
&amp;= U^TQ\Lambda Q^TU\\
&amp;= [I_k, 0_{n-k}]\Lambda [I_k, 0_{n-k}]^T&amp;\text{orthogonality}\\
&amp;= \text{Top left } K\times K\text{ block of }\Lambda 
\end{align*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Autoencoder">Autoencoder<a class="anchor-link" href="#Autoencoder">&#182;</a></h2><p>An autoencoder is a feed-forward neural net to take input/target pair $(\vec x, \vec x)$. In such NN, we add a bottleneck layer to reduce the dimensionality so that the weights on such layer will be our code vector.</p>
<p>The whole process goes through 
$$x \Rightarrow Encode \Rightarrow Bottleneck(code\_vector)\Rightarrow Decode\Rightarrow \tilde x$$</p>
<p>By doing such, we learn abstract features in an unsupervised way, and can transfer to supervised tasks.</p>
<h4 id="Linear-autoencoders">Linear autoencoders<a class="anchor-link" href="#Linear-autoencoders">&#182;</a></h4><p>If we use linear activations and squared error loss. Say we have 1 hidden layer of $k$ weights, so that $\tilde x = W_2W_1x, W_2:D\times K, W_1: K\times D$, then $W_2$ is the PCA subspace</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Nonlinear-Autoencoder">Nonlinear Autoencoder<a class="anchor-link" href="#Nonlinear-Autoencoder">&#182;</a></h3><p>If we use non-linear activations, then they can be more powerful for a given dimensionality, comparing to PCA (but also much more computational heavy in finding an optimal subspace).</p>

</div>
</div>
</div>
    </div>
  </div>
</section>
  <script src="https://lihd1003.github.io/assets/js/vendor.js"></script>
  <script src="https://lihd1003.github.io/assets/js/app.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script>
    $("table").addClass("table table-lined")
  </script>
</body>

 


</html>
