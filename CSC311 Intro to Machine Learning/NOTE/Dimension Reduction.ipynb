{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathcal D = \\{x^{(1)},...,x^{(N)}\\}\\subset \\mathbb R^D$, so that $N$ instances will form matrix \n",
    "$$X = \\begin{bmatrix}[\\vec x^{(1)}]^T\\\\...\\\\ [\\vec x^{(N)}]^T\\end{bmatrix}$$\n",
    "each row will be one observation of $D$ faetures,  \n",
    "Let $x^{(i)}\\sim N(\\mu, \\Sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction\n",
    "Loss some information (e.g. spread / $\\sigma$) by projecting higher dimensions onto lower ones. IN practice, the important features can be accurately captured in a low dimensional subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection onto a subspace\n",
    "Given $\\mathcal D, \\hat\\mu=N^{-1}\\sum^N x^{(i)}$ be the sample mean.  \n",
    "\n",
    "Want to find a $K <D $ dimensional subspace $S\\subset \\mathcal R^D$ s.t. $x^{(n)}-\\hat\\mu$ is \"well represented\" by a projection onto $K$-dimensional $S$. \n",
    "\n",
    "Where __projection__ is to find the closest point $\\tilde x$ on $S$ s.t. $\\|x-\\tilde x\\|$ is minimized. \n",
    "\n",
    "In a 2-dimensional problem, we are looking for direction $u_1$ along with the data is well-represented, such as direction of higher variance or the direction of min difference after projection, which turns to be the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean Projection\n",
    "\n",
    "$Proj_S(x):=$ projection of $x$ on $S$. \n",
    "\n",
    "In 2D case, $S$ is the line along the unit vector $u$ (1-D subspace). $u$ is a basis for $S$. \n",
    "\n",
    "Since $x^Tu = \\|x\\|\\|u\\|\\cos\\theta =\\|x\\|\\cos\\theta$\n",
    "$$Proj_S(x) = x^Tu\\cdot u = \\|\\tilde x\\|u$$\n",
    "\n",
    "In K-D case, we have $K$ basis $u_1,...,u_K \\in \\mathcal R^D$. and the projection will be \n",
    "$$Proj_S(x) = \\sum^K (x^Tu_i) u_i = \\sum^K z_i u_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Center data\n",
    "Centering by subtract the mean $\\hat\\mu$. i.e. the mean (center) be the origin. We need to center the data since we don't want location of data to influence the calculations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representation/code\n",
    "Combine the two above together, we have \n",
    "$$\\tilde x = \\hat\\mu + Proj_S(x-\\hat\\mu) = \\hat\\mu + \\sum^K z_i \\vec u_i$$\n",
    "where $z_k = \\vec u_k^T(x-\\hat\\mu)$  \n",
    "\n",
    "Define matrix $U_{D\\times K} = [\\vec u_1, ..., \\vec u_K]$, then $$\\vec z = U^T(x-\\hat\\mu), \\tilde x = \\hat\\mu + U\\vec z = \\hat\\mu + UU^T(x-\\hat\\mu)$$ \n",
    "We call $UU^T$ the projector on $S$, $U^TU = I$\n",
    "\n",
    "Both $x,\\tilde x\\in \\mathbb R^D$ but $\\tilde x$ is a linear combination of vectors in a lower dimensional subspace with representation $\\vec z \\in \\mathbb R^K$. \n",
    "\n",
    "We call $\\tilde x$ __reconstruction__ of $x$, $\\vec z$ be its representation(code). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a Subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will definitely lose partial information by dimension reduction, we want a good $D\\times K$ matrix $U$ with orthonormal columns. \n",
    "\n",
    "To measure how \"good\" the subspace is, propose two criteria:\n",
    "\n",
    "__Minimize reconstruction error__: find vectors in a subspace that are closest to data points \n",
    "$$\\arg\\min_U \\frac 1 N \\sum^N \\|x^{(i)} - \\tilde x^{(i)}\\|^2$$\n",
    "__Maximize variance of reconstructions__ find a subspace where data has the most variability\n",
    "$$\\arg\\max_U \\frac 1 N \\sum^N \\|\\tilde x^{(i)} - \\hat\\mu\\|^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noting that \n",
    "\\begin{align*}\n",
    "E(\\tilde x) &= E(\\hat\\mu + UU^T(x-\\hat\\mu))\\\\\n",
    "&= \\hat\\mu + UU^T(E(x)-\\hat\\mu)\\\\\n",
    "&= \\hat\\mu + UU^T0\\\\\n",
    "&= \\hat\\mu\n",
    "\\end{align*}\n",
    "So that we can still use mean of $x$ to calculate variance of the reconstruciton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equivalence of the criteria\n",
    "__lemma1__ Norm of centered reconstruction is equal to norm of representation\n",
    "\\begin{align*}\n",
    "\\|\\tilde x^{(i)} - \\hat\\mu\\|^2 &= (U\\vec z^{(i)})^T(U\\vec z^{(i)})\\\\\n",
    "&=  (\\vec z^{(i)})^T U^TU\\vec z^{(i)}\\\\\n",
    "&= (\\vec z^{(i)})^T\\vec z^{(i)}&U^TU = I\\\\\n",
    "&= \\|z^{(i)}\\|\n",
    "\\end{align*}\n",
    "\n",
    "__lemma2__ $\\tilde x^{(i)}-\\hat\\mu$ is orthogonal to $\\tilde x^{(i)} - x^{(i)}$\n",
    "\n",
    "\\begin{align*}\n",
    "(\\tilde x^{(i)}-\\hat\\mu)^T(\\tilde x^{(i)} - x^{(i)}) &= (\\hat\\mu+UU^T(x^{(i)}-\\hat\\mu)-\\hat\\mu)^T(\\hat\\mu+UU^T(x^{(i)}-\\hat\\mu) - x^{(i)})\\\\\n",
    "&= (x^{(i)}-\\hat\\mu)^TUU^T(\\hat\\mu- x^{(i)}+UU^T(x^{(i)}-\\hat\\mu) )\\\\\n",
    "&= (x^{(i)}-\\hat\\mu)^TUU^T(\\hat\\mu - x^{(i)}) + (x^{(i)}-\\hat\\mu)^TUU^TUU^T(x^{(i)}-\\hat\\mu))\\\\\n",
    "&= (x^{(i)}-\\hat\\mu)^TUU^T(\\hat\\mu - x^{(i)}) + (x^{(i)}-\\hat\\mu)^TUU^T(x^{(i)}-\\hat\\mu))\\\\\n",
    "&= 0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Proposition__ The two criteria is equivalent $\\frac 1 N \\sum^N\\|x^{(i)}- \\tilde x^{(i)}\\|^2 = C - \\frac1N\\sum^N\\|\\tilde x^{(i)}-\\hat\\mu\\|^2$\n",
    "\n",
    "By lemma2, since the two vectors are orthogonal, by Pythagorean Theorem\n",
    "\\begin{align*}\n",
    "\\|\\tilde x^{(i)} - \\hat\\mu\\|^2 + \\|x^{(i)} - \\tilde x^{(i)}\\|^2 &= \\|x^{(i)}-\\hat\\mu\\|^2\\\\\n",
    "\\frac1N\\sum^N \\|\\tilde x^{(i)} - \\hat\\mu\\|^2 + \\frac1N\\sum^N\\|x^{(i)} - \\tilde x^{(i)}\\|^2 &= \\frac1N\\sum^N\\|x^{(i)}-\\hat\\mu\\|^2\\\\\n",
    "\\text{projected variance} + \\text{reconstruction error} &= C\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "#### Spectral Decomposition (Eigendecomposition)\n",
    "If $A_{n\\times n}$ is a symmetric matrix (so that has a full set of eigenvectors). Then, $\\exists Q_{n\\times n}, \\Lambda_{n\\times n}$ s.t. $A = Q\\Lambda Q^T$ where $Q$ is orthogonal matrix formed by $n$ eigenvectors and $\\Lambda$ is diagonal with $\\lambda_1,...,\\lambda_n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Eigendecomposition on the __empirical convariance matrix__ $\\hat\\Sigma = \\frac1N \\sum^N (x^{(i)}-\\hat\\mu)(x^{(i)}-\\hat\\mu)^T$, the optimal PCA subspace is then spanned by some $K$ eigenvectors of $\\hat\\Sigma$\n",
    "\n",
    "These eigencectors are called principal components, analogous to the principal axes of an ellipse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deriving PCA for $K=1$\n",
    "\\begin{align*}\n",
    "\\frac 1N \\sum^N\\|\\tilde x^{(i)} - \\hat\\mu\\|^2 &= \\frac1N\\sum^N [z^{(i)}]^2\\\\\n",
    "&= \\frac1N\\sum^N(u^T(x^{(i)}-\\hat\\mu))^2\\\\\n",
    "&=  \\frac1N\\sum^Nu^T(x^{(i)}-\\hat\\mu)(x^{(i)}-\\hat\\mu)^Tu\\\\\n",
    "&= u^T\\bigg[\\frac1N\\sum^N(x^{(i)}-\\hat\\mu)(x^{(i)}-\\hat\\mu)^T\\bigg]u\\\\\n",
    "&= u^T\\hat\\Sigma u= u^TQ\\Lambda Q^Tu=a^T\\Lambda a= \\sum^D\\lambda_j a_j^2\n",
    "\\end{align*}\n",
    "For the goal of maximize $\\sum^D \\lambda_j a_j^2, \\vec a = Q^Tu$, noting that $\\sum a_j = a^Ta = u^TQQ^Tu = u^Tu = 1$. Therefore, choosing the largest $\\lambda_k$, \n",
    "$$\\sum \\lambda_ja_j^2 \\leq \\sum \\lambda_k a_j^2 = \\lambda_d\\sum a_j^2 = \\lambda_k$$\n",
    "And such maximum can be obtained by setting $a_i = \\mathbb I(i = k)$. Therefore, $\\vec u = Q\\vec a = q_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decorrelation\n",
    "\\begin{align*}\n",
    "cov(\\vec z) &= cov(U^T(x-\\mu))\\\\\n",
    "&= U^Tcov(x)U\\\\\n",
    "&= U^TQ\\Lambda Q^TU\\\\\n",
    "&= [I_k, 0_{n-k}]\\Lambda [I_k, 0_{n-k}]^T&\\text{orthogonality}\\\\\n",
    "&= \\text{Top left } K\\times K\\text{ block of }\\Lambda \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder\n",
    "An autoencoder is a feed-forward neural net to take input/target pair $(\\vec x, \\vec x)$. In such NN, we add a bottleneck layer to reduce the dimensionality so that the weights on such layer will be our code vector. \n",
    "\n",
    "The whole process goes through \n",
    "$$x \\Rightarrow Encode \\Rightarrow Bottleneck(code\\_vector)\\Rightarrow Decode\\Rightarrow \\tilde x$$\n",
    "\n",
    "By doing such, we learn abstract features in an unsupervised way, and can transfer to supervised tasks. \n",
    "\n",
    "#### Linear autoencoders\n",
    "If we use linear activations and squared error loss. Say we have 1 hidden layer of $k$ weights, so that $\\tilde x = W_2W_1x, W_2:D\\times K, W_1: K\\times D$, then $W_2$ is the PCA subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear Autoencoder\n",
    "If we use non-linear activations, then they can be more powerful for a given dimensionality, comparing to PCA (but also much more computational heavy in finding an optimal subspace)."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
