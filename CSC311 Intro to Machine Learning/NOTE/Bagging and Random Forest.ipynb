{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "### Intuition\n",
    "Suppose we sample $m$ indep. training set $D_i$ from $p_d$, we could then learn a predictor $h_i := h_{D_i}$ based on each one, then take the average $h = \\sum^m h_i /m$ \n",
    "\n",
    "Bias unchanged\n",
    "$$E_{D_1,...,D_m \\sim p_d}h(x) = \\frac{1}{m} \\sum^m E_{D_i\\sim p_{d}}h_i(x) = E_{D\\sim p_d} h_D(x)$$\n",
    "\n",
    "Variance becomes $1/m$ of the original \n",
    "$$var_{D_1,...,D_m}(h(x)) = \\frac{1}{m^2}var_{D_i}(h_i(x)) = \\frac{1}{m}var_D(h_D(x))$$\n",
    "\n",
    "However, we don't such iid datasets from $p_d$\n",
    "\n",
    "So we have to take a single dataset $D$ with $n$ examples  \n",
    "Generate $m$ new datasets by sampling $n$ training examples from $D$, with replacement  \n",
    "Average the predictions of models trained on each of these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem with independence\n",
    "Let correlation be $\\rho$, the variance with correlated datasets is \n",
    "$$var(h(x)) = \\frac{1}{m}(1-\\rho)\\sigma^2 + \\rho\\sigma^2$$\n",
    "\n",
    "Ironically, introduce additional variability reduces correlation between samples  \n",
    " - invest a diversified portfolio, not just one stock\n",
    " - help to use average over multiple algorithms, or multiple configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "#### General Idea\n",
    "When choose each node of the decision tree, choose a random set of $d$ features, and only consider splits on those features\n",
    "\n",
    "The main idea is to improve the variance reduction of bagging by reducing the correlation between the trees\n",
    "\n",
    "One of the example for black-box ML algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Optimality\n",
    "What if $t$ is not deterministic given $\\vec x$, i.e. $p(t\\mid \\vec x)$. \n",
    "\n",
    "Since there is a distribution over targets, we measure distance from $y_*(x) = E(t\\mid \\vec x)$  \n",
    "$$E[(y-t)^2 \\mid \\vec x] = (y - y_*(\\vec x))^2 + var[t\\mid \\vec x]$$  \n",
    "The first term show that s $y=y_*(\\vec x)$ is the minimized value  \n",
    "\n",
    "__Bayes error / irreducible error__ The second term is the inherent unpredicatability, or noise, of the target. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
