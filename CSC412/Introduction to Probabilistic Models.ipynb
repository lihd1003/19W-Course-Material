{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Probabilistic Models\n",
    "\n",
    "Let r.v. $\\mathcal X = (X_1, ..., X_M)$ with the model relationships $p_\\theta(\\mathcal X)$. \n",
    "\n",
    "Assuming a generative distribution, i.e. $(X_1,...,X_M) \\sim p_{data}(\\mathcal X)$\n",
    "\n",
    "Then, the learning goal is to find best $p_{\\theta^*} \\approx p_{data}$. \n",
    "Therefore, the problem is to define \"best\" and methods to find the \"best\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Probabilistic Perspective on ML Tasks\n",
    "\n",
    "### Supervised\n",
    "Assuming the input data $X$, labels $C$, and continuous output $y$. \n",
    "\n",
    "Given $\\mathcal D = \\{(x,c)_1,...,(x,c)_M\\}$, assume $(x,c)\\sim p_{data}(X,C)$  \n",
    "__classification__ Let $c$ be discrete $$p(C\\mid X) = \\frac{p(X,C)}{p(X)} = \\frac{p(X,C)}{\\sum_{c\\in C} p(X,c)}$$\n",
    "\n",
    "\n",
    "__regression__ Let $y$ be continuous \n",
    "$$p(Y\\mid X) = \\frac{p(X,Y)}{p(X)} = \\frac{p(X,Y)}{\\int_Y p(X,y)dy}$$\n",
    "\n",
    "### Unsupervised\n",
    "$\\mathcal D = \\{(x)_1,...,(x)_M\\}$, assuming there exists some features (e.g. clustering) i.e. assuming $X, \\_ \\sim p(X,C_{hidden})$  \n",
    "Then, we still want to transfer the question into $p_\\theta (C\\mid X)$\n",
    "\n",
    "### Semi-supervised\n",
    "When labels are partially observed, for example, matrix completion problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Variables\n",
    "Never observed, encode modeling assumptions, infer complex features\n",
    "\n",
    "### Operations on the probabilistic models\n",
    "Generating data $x\\sim p_\\theta(x)$  \n",
    "Estimating likelihood  \n",
    "Inferencing  \n",
    "Learning the parameters\n",
    "\n",
    "### Considerations on Prob. Models\n",
    "efficient computations  \n",
    "compact representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality of Joint Distribution\n",
    "Suppose we have variables $T=\\{t_0,t_1\\},W=\\{w_0, w_1\\},M = \\{m_0, m_1\\}$, then the joint distribution $p(T,W,M)$ is parameterized with $2\\times 2 \\times 2= 8$ parameters. Hence if we add more states onto each variables, say we have $k$ states for each of $n$ variables. Then, the join distribution will have $k^n$ parameters. \n",
    "\n",
    "#### Reduce Dimensionality of Join Distribution\n",
    "With assumptions on the independence of variables, then we can reduce the dimensionality and reduce the number of parameters. \n",
    "\n",
    "note that __fully factorized__ means that \n",
    "$$p(T,W,M) = p(T)p(W\\mid T)p(M\\mid T,W) = p(M)p(T\\mid M)p(W\\mid T,M) =...$$\n",
    "\n",
    "suppose we have the assumption that $T,W$ are independent, then the factorization "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
