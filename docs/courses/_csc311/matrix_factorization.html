---
title: Matrix Factorization
order: 100
---

<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sn</span>
</pre></div>


<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1>PCA as a matrix factorization</h1>
<p>Given PCA reconstruction 
$$x^{(i)}\approx Uz^{(i)}$$
Write all the observations into matrix form, 
$$\underset{X\in \mathbb R^{N\times D}}{\begin{bmatrix}[x^{(1)}]^T \\ \vdots \\ [x^{(N)}]^T
\end{bmatrix}}\approx 
\underset{Z\in\mathbb R^{N\times K}}{\begin{bmatrix}[z^{(1)}]^T \\ \vdots \\ [z^{(N)}]^T 
\end{bmatrix}}U^T$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Using <strong>Frobenius norm</strong> $\|Y\|_F^2 = \|Y^T\|^2_F = \sum_{i,j}y_{ij}^2 = \sum_i \|y^{(i)}\|^2$, define the squared error being $\|X-ZU^T\|_F^2$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2>Singular Value Decomposition</h2>
<p>Given $X$, the SVD is $X = QSU^T$ where<br>
$Q\in\mathbb R^{N\times D}$ with orthonormal columns, i.e. $Q^TQ = I_D$.<br>
$S\in\mathbb R^{D\times D}$ is the diagonal matrix<br>
$U\in\mathbb R^{D\times D}$ is the orthonormal matrix, $U^T = U^{-1}$</p>
<h3>Properties of covariance matrices</h3>
<p>Construct two positive semi-definite matrices $XX^T (N\times N)$ and $X^TX(D\times D)$<br>
$XX^T = QSU^T(QSU^T)^T = QSU^TUSQ^T = QSISQ^T = QS^2Q^T$ is an eigendecomposition of $XX^T$, similarly $X^TX = US^2U^T$<br>
Assuming $N\geq D$, then $XX^T,X^TX$ will share $D$ eigenvalues and the remaining $N-D$ eigenvalues will be $0$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Therefore, consider $\hat\Sigma$, 
\begin{align*}
\hat\Sigma &amp;= N^{-1}X^TX \\
&amp;= N^{-1}USQ^TQSU^T\\
&amp;= U(S^2/N)U^T
\end{align*}
the eigenvalues $\lambda_i$ are related to the singular values $\lambda_i = s_i^2 / N$, the SVD gives $U$ which is equivalent to the learned basis of PCA.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1>Recommender Systems (Matrix Completion)</h1>
<p>Compare to a PCA problem, where all the observations are known. Recommender systems are often sparse matrices with many unknown, i.e. Matrix completion problem.</p>
<p>Consider the movie rating problem, which given users, movies, and user rates on some pf the movies. Want to predict the ratings of the movies that the user haven't watched.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2>Alternating Least Squares</h2>
<p>Let the representation of user $n$ in the $K-$dimensional space be $\vec u_n$ and the representation of movie $m$ be $z_m$. Assume the rating user $n$ gives to movie $m$ is given by $R_{nm}\approx u_n^T z_m$, then this gives 
$$R = \underset{U}{\begin{bmatrix}u_1^T\\\vdots\\u_N^T\end{bmatrix}}\underset{Z^T}{\begin{bmatrix}z_1&amp;...&amp;z_M\end{bmatrix}}$$</p>

</div>
</div>
</div>

<div class=" highlight hl-ipython3"><pre><span></span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">K</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">3</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>
<span class="n">U</span><span class="p">[</span><span class="n">U</span> <span class="o">&lt;</span> <span class="mf">2.5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">Z_T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">M</span><span class="p">))</span>
<span class="n">Z_T</span><span class="p">[</span><span class="n">Z_T</span><span class="o">&lt;</span><span class="mf">2.5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;R&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">U</span><span class="nd">@Z_T</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Greens&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;U&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Greens&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Z_T&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">Z_T</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Greens&quot;</span><span class="p">);</span>
</pre></div>


<div class="output_wrapper">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To enforce $R\approx UZ^T$, i.e. $\min_{U,Z} \sum _{i,j}(R_{ij} - u_i^Tz_j)^2$, while most $R_{ij}$ are unknown, so we can only minimize the observed parts, i.e. 
$$\min_{U,Z} \sum_{(n,m)\in \mathcal O} (R_{nm}- u_n^Tz_m)^2\|, \mathcal O := \{(n,m), R_{nm}\text{ is observed}\}$$
However, such objective is non-convex and generally NP-hard to solve, so we have to use alternating approach, i.e. optimize $U,Z$ individually and alternatively.</p>
<p>The generally algorithm follows that</p>
<ul>
<li>initialize $U,Z$ randomly</li>
<li>repeat until convergence <ul>
<li>for $n = 1..N$
$$u_n = (\sum_mz_mz_m^T)^{-1}\sum_m R_{nm}z_m$$</li>
<li>for $m = 1..M$
$$z_m = (\sum_n u_n u_n^T)^{-1}\sum_n R_{nm}u_n$$</li>
</ul>
</li>
</ul>
<p>where $(n,m)\in \mathcal O$.</p>
<p>With such algorithm, we can do gradient descent<br>
Full gradient descent is $\begin{bmatrix}U\\Z\end{bmatrix} \leftarrow \begin{bmatrix}U\\Z\end{bmatrix} - \alpha \nabla f(U,Z)$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For large dataset, we can use stochastic gradient descent $$\begin{bmatrix}u_n\\z_m\end{bmatrix} \leftarrow \begin{bmatrix}u_n\\z_m\end{bmatrix} - \alpha \begin{bmatrix}(R_{nm}-u_n^Tz_m)z_m\\(R_{nm}-u^T_nz_m)u_n\end{bmatrix}$$</p>
<p>So we can change the updating step to</p>
<ul>
<li>randomly select a pair $(n,m)\in\mathcal O$ among observed elements of $R$. </li>
<li>$u_n \leftarrow u_n - \alpha (R_{nm} - u_n^T z_m)z_m$</li>
<li>$z_m \leftarrow z_m - \alpha (R_{nm} - u_n^Tz_m)u_n$</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1>K-Means as a matrix factorization</h1>
<p>Stack assignment vectors $r_i$ into $N\times K$ matrix R, and the cluster means $m_k$ into a matrix $K\times D$ matrix $M$. Then, the reconstruction with its cluster is given by $RM$.</p>
<p>Taking one step further, feature dimensions can be redundant and some feature dimensions cluster together.</p>
<p><strong>Co-clustering</strong> clusters both the rows and columns of a data matrix, giving a block structure. We can represent this as the indicator matrix for rows, times the matrix of means for each block, times the indicator matrix for columns.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2>Sparse Coding</h2>
<p>This algorithm works on small image patches, which we reshape into vectors.<br>
Suppose we have a dictionary of <strong>basis functions</strong> $\{a_k\}_{k=1}^K$ which can be combined to model each patch.<br>
Each patch is approximated as a linear combination of a small number of basis functions 
$$x = \sum^K s_k a_k = As$$
This is an overcomplete representation, in that typically $K&gt;D$ for sparse coding problems, the requirement that $s$ is sparse makes things interesting.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'd like choose $s$ to accurately reconstruct the image $x\approx As$ but encourage sparsity in $s$.<br>
What cost function should we use?<br>
Inference in the sparse coding model: 
$$\min_s \|x-As\|^2 + \beta \|s\|_1$$
Here, $\beta$ is a hyper-parameter that trades off reconstruction error v. sparsity.<br>
There are efficient algorithms for minimizing this cost function.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2>Learning the dictionary</h2>
<p>We can learn a dictionary by optimizing both $A$ and $\{s_i\}_{i=1}^N$ to trade off reconstruction error and sparsity 
$$\min_{\{s_i\}, A} \sum_{i=1}^N \|x^{(i)}- As_i\|^2 + \beta\|s_i\|_1$$
subject to $\|a_k\|^2 \leq 1, \forall k$.<br>
Reconstruction term can be written in matrix form as $\|X-AS\|_F^2$ where $S$ combines the $s_i$ as columns.<br>
Can fit using an alternating minimization scheme over $A$ and $S$ just like $K$-means, $EM$, low-rank matrix completion, etc.</p>
<p>The sparse components are oriented edges, similar to what a neural networks learn.<br>
But the learned dictionary is much more diverse than the first-layer neural net representations. Tiles the space of location, frequency, and orientation in an efficient way.</p>

</div>
</div>
</div>
 

