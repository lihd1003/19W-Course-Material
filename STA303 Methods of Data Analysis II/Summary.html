<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />

<title>Summary - Notes Portal</title>

<link rel="icon" type="image/gif" href="https://lihd1003.github.io/assets/images/logo.png">
<link rel="stylesheet" href="https://lihd1003.github.io/assets/css/vendor.css" />
<link rel="stylesheet" href="https://lihd1003.github.io/assets/css/style.css" />
<link href="https://fonts.googleapis.com/css?family=Roboto&display=swap" rel="stylesheet">
<script src="https://kit.fontawesome.com/98fa07784c.js"></script>



<style type="text/css">
/* Overrides of notebook CSS for static HTML export */

div#notebook {
  overflow: visible;
  border-top: none;
}@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="https://lihd1003.github.io/UofT-Course-Material-Repo/index/custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
	<!-- header -->
   <header class="header-sticky header-dark">
    <div class="container">
      <nav class="navbar navbar-expand-lg navbar-dark">
        <a class="navbar-brand" href="./index.html">
          <img class="navbar-logo navbar-logo-light" src="https://lihd1003.github.io/assets/images/logo.png" alt="Logo">
          <img class="navbar-logo navbar-logo-dark" src="https://lihd1003.github.io/assets/images/logo.png" alt="Logo">
        </a>

        <div class="collapse navbar-collapse" id="navbarSupportedContent">
          <ul class="navbar-nav align-items-center mr-auto">
          </ul>

          <ul class="navbar-nav align-items-center mr-0">
            <li class="nav-item">
              <a href="https://github.com/lihd1003" class="nav-link waves-effect waves-light" target="_blank">
                <i class="icon-github mr-2"></i>GitHub
              </a>
            </li>
            <li class="nav-item">
              <a href="https://github.com/lihd1003/UofT-Course-Material-Repo" class="nav-link waves-effect waves-light" target="_blank">
                <i class="icon-star mr-2"></i>Star the Repo
              </a>
            </li>
          </ul>
        </div>
      </nav>
    </div>
  </header>

<section class="hero hero-with-header text-white" data-top-top="transform: translateY(0px);" data-top-bottom="transform: translateY(250px);">
    <div class="image image-overlay" style="background-image:url(https://lihd1003.github.io/assets/images/black.jpg)"></div>
    <div class="container">
      <div class="row align-items-center">
        <div class="col text-shadow">

          <h1 class="mb-0">Summary</h1>
        </div>
      </div>
    </div>
 </section>
 <section class="bg-white sec" id="project">
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>pooled two sample t-tests</p>
<ul>
<li>assume equal population variances</li>
<li>$s^2_p = \frac{(n_x-1)s_x^2 + (n_y-1)s_y^2}{n_x+n_y-2}$</li>
<li>$t = \frac{(\bar{x} - \bar{y} -D_0)}{\sqrt{s_p^2(n_x^{-1}+n_y^{-1})}}\sim t_{(n_x-1)(n_y-1)}$</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="SLM-Dummy-variable">SLM Dummy variable<a class="anchor-link" href="#SLM-Dummy-variable">&#182;</a></h4><p>$Y_i=\beta_0+\beta_1X_i + \epsilon_i$
assumptions</p>
<ul>
<li>linear model is appropriate</li>
<li>$\epsilon_i \sim N(0,\sigma^2)$</li>
</ul>
<p>Hypothesis test:<br>
 $H_0:\beta_1 = 0, H_a:\beta_1\neq 0$<br>
$t=b_1/se(b_1)\sim t_{N-2}$, $N$ is the total number of observations</p>
<h4 id="One-Way-ANOVA--&amp;-GLM">One Way ANOVA  &amp; GLM<a class="anchor-link" href="#One-Way-ANOVA--&amp;-GLM">&#182;</a></h4><p>$Y_i=\vec{X_i}\vec{\beta}+\vec{\epsilon}$</p>
<ul>
<li>assumptions: same as dummy variable, jointly normally distributed errors</li>
<li>$F=MSReg/MSE = \frac{(SSR/G-1)}{SSE/(N-G)}\sim F_{G-1,N-G}$</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Multiple-Comparisons">Multiple Comparisons<a class="anchor-link" href="#Multiple-Comparisons">&#182;</a></h4><p>Bonferrroni's Method:</p>
<ul>
<li>$P(\cup A_i)\leq \sum P(A_i)$</li>
<li>$k= {G \choose 2}$</li>
<li>level at $a/k$</li>
</ul>
<p>Tukey's Method: less conservative than Bonferroni's method</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Two-Way-ANOVA">Two Way ANOVA<a class="anchor-link" href="#Two-Way-ANOVA">&#182;</a></h4><p>Overall vs. Partiral F-tests<br>
$H_0: $ a subset of $\beta$'s are 0. $H_a: $ some of the $\beta$ in the subset are not 0.<br>
Let FULL model be with all explanatory variables, REDUCED be without the coefficients in testing.<br>
$F=\frac{(RSS_r - RSS_f)/\# \beta \text{ tested}}{MSE_f}\sim F_{\# \beta \text{ tested}, d.f. RES_f}$</p>
<p>Describing "interactions"</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="GLM-vs.-Transformation">GLM vs. Transformation<a class="anchor-link" href="#GLM-vs.-Transformation">&#182;</a></h4><p>Transform Y so it has an approximate normal  distribution with constant variance,</p>
<p>GLM: distribution of Y not restricted to Normal,<br>
model parameters describe $g(E(Y))$ rather than $E(g(Y))$<br>
GLMs provide a unified theory of modeling that encompasses the most important models for continuous and discrete variables.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="GLM-tests">GLM tests<a class="anchor-link" href="#GLM-tests">&#182;</a></h4><p>Wald<br>
$H_0:\beta_j = 0, H_a: \beta_j\neq 0$<br>
$z=\hat\beta_j / se(\hat\beta_j)\sim N(0,1)$.<br>
CI: $\hat\beta_j \pm z_{a/2} se(\hat\beta_j)$</p>
<p>LRT<br>
$H_0: $ some $beta$ are 0, $H_a: $ at least one tested $\beta$ is not 0.</p>
$$G^2 = (-2\log \mathcal L_R) - (-2\log \mathcal L_F) = -2\log (\mathcal L_R / \mathcal L_F)\sim \chi^2_k$$<p>$k= \# \beta$ tested</p>
<p>Global LRT<br>
LRT comparing to the NULL model (null deviance)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="AIC,-BIC">AIC, BIC<a class="anchor-link" href="#AIC,-BIC">&#182;</a></h4><ul>
<li>combines log-likelihood with a penalty </li>
<li>$AIC = -2\log\mathcal L + 2(p+1)$</li>
<li>$BIC =  -2\log\mathcal L + \log N(p+1)$</li>
<li>$p$ number of explanatory variables, $N$ sample size</li>
<li>Smaller is better</li>
<li>Better = $diff(AIC) &gt; 10$</li>
<li>Same = $diff(AIC) &lt; 2$</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="SLR-vs.-Binary-LR">SLR vs. Binary LR<a class="anchor-link" href="#SLR-vs.-Binary-LR">&#182;</a></h4><ul>
<li><p>both use MLE</p>
</li>
<li><p>Binary LR has fewer assumptions</p>
<ul>
<li>no outelires</li>
<li>no residual plots</li>
<li>non constant variance </li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Binary-Logistic-Regression">Binary Logistic Regression<a class="anchor-link" href="#Binary-Logistic-Regression">&#182;</a></h4><p>underlying distribution for each independent observation: $Bernoulli(\pi_i)$</p>
<p>We cannot estimate $\pi_i$ for individual $i$.</p>
<ul>
<li>Let $\pi = P(success)$, </li>
<li>ODDS: $\pi/(1-\pi)$</li>
<li>LOG ODDS: $\log(\pi/(1-\pi))$</li>
<li>ODDS RATIO is the ratio of two ODDS</li>
</ul>
<p>$E(Y\mid X)=\pi, var(Y\mid X) = \pi(1-\pi)$</p>
<p>The model
$$\log(\pi/(1-\pi)) = X\beta$$
$$\log(\frac{\pi_i}{1-\pi_i}) = X_i\beta$$ (no error term)</p>
<p>MLE:
$P(Y_i=y_i)=\pi_i^{y_i}(1-\pi_i)^{1-y_i}$
$$\mathcal{L} = \prod_1^n\pi_i^{y_i}(1-\pi_i)^{1-y_i}$$
where $\pi_i = \frac{\exp(X_i\beta)}{1+\exp(X_i\beta)}=e^{\mu}/(1+e^\mu)$
and $$1-\mu_i = 1-\frac{e^\mu}{1+e^\mu} = (1+e^\mu)^{-1}$$</p>
$$\log\mathcal{L} = \\
\sum_1^ny_i(X_i\beta) - y_i\log(1+\exp(X_i\beta))-(1-y_i)\log(1+X_i\beta))$$<p>Let $(a,b)$ be CI, CI for Odds ratio is $e^a, e^b$, while we cannot compute CI for $\pi$ since $\pi$ is not normally distributed</p>
<p>Assumptions:</p>
<ul>
<li>underlying model for Y is Bernoulli</li>
<li>independent observations</li>
<li>Correct form of model (linear relationship, included all relevant variables and excluded irrelevant)</li>
<li>enough large sample size</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Binomial-Logistic-Regression">Binomial Logistic Regression<a class="anchor-link" href="#Binomial-Logistic-Regression">&#182;</a></h4><p>Let $Y$ be the count of the number of "success"</p>
<p>$P(Y=y)={m\choose y}\pi^y (1-\pi)^{m-y}$</p>
<p>$E(Y)=m\pi, var(Y)=m\pi(1-\pi)$</p>
<p>Then the proportion of successes 
$E(Y/m)=\pi, var(Y/m)=\pi(1-\pi)/m$</p>
<p>Assume for each group of observation, it is independent.</p>
<p>We can estimate $\pi_i$ is this case</p>
<p>MLE:<br>
$$P(Y_i=y_i) = {m_i\choose y_i}\pi^{y_i}(1-\pi_i)^{m_i-y_i}$$
$$\mathcal L = \prod_1^n {m_i\choose y_i}\pi^{y_i}(1-\pi_i)^{m_i-y_i}$$ where $\pi_i = \frac{e^\mu}{1+e^\mu}$
$$\log\mathcal L = \sum y_i\log(\pi_i)+(m_i-y_i)\log(1-\pi_i) + \log{m_i\choose y_i}$$</p>
<p>Deviance $=-2\log(\mathcal L_M/\mathcal L_S) = -2(\log \mathcal L_M - \log \mathcal L_S)$.</p>
<p>Saturated model has log likelihood ratio 0.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Logistic-Regression-Problems">Logistic Regression Problems<a class="anchor-link" href="#Logistic-Regression-Problems">&#182;</a></h4><ul>
<li>Extrapolation: model outside of range of observed data may not be appropriate</li>
<li>Multicollinearity<ul>
<li>unstable fitted equation</li>
<li>coefficient significance and signs</li>
<li>large standard error of coefficients</li>
<li>MLR may not converge</li>
</ul>
</li>
<li>Influential points</li>
</ul>
<p>Specific to logistic</p>
<ul>
<li>Complete separation<ul>
<li>one of a linear combination of explanatory variables perfectly predict $Y$, then MLE cannot be computed</li>
</ul>
</li>
<li>Quasi-complete separation<ul>
<li>almost perfectly predict Y
<strong>Solution</strong> simplify model, or try other options</li>
</ul>
</li>
</ul>
<p>Extra-binomial variation</p>
<ul>
<li>when Bernoulli observations are not independent</li>
<li>use quasibinomial </li>
<li>model for variance: $var(Y_i)=\phi m_i \pi_i(1-\pi_i)$</li>
<li>$\hat\phi = $ sum of squared Pearson residuals / d.f. </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="GOF">GOF<a class="anchor-link" href="#GOF">&#182;</a></h4><p>To check model adequacy using LRT</p>
<p>$H_0: $ fitted model fits data as well as Saturated model. $H_a: $ saturated model is better, the fitted model is inadequate</p>
<p>$G^2 = -2\log(\mathcal L_F /\mathcal L_S)\sim \chi^2_{n-(p+1)}$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Log-linear-Model">Log linear Model<a class="anchor-link" href="#Log-linear-Model">&#182;</a></h4><ul>
<li>Why not linear<ul>
<li>outcome is counts and small numbers</li>
<li>Won't have a normal distribution conditional on age</li>
</ul>
</li>
<li>Why no logistic<ul>
<li>Not a binary outcome</li>
<li>Not a binomial outcome since not a fixed number of trials</li>
</ul>
</li>
</ul>
<p>$P(Y=y)=\mu^y e^{-\mu} / y!, E(Y)=var(Y)=\mu$</p>
$$\mathcal L = \prod_1^n \mu_i^{y_i} e^{-\mu_i} / y_i!$$$$\log\mathcal L = \sum_1^n y_i \log (\mu_i) -\mu_i - \log(y_i!)$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Two-Factor-Independence">Two Factor Independence<a class="anchor-link" href="#Two-Factor-Independence">&#182;</a></h4><p><strong>Binomial Sampling</strong><br>
For $2\times 2$ table<br>
$H_0: \mu_a = \mu_b, H_a: \mu_a\neq \mu_b$
$$z=\frac{\hat\mu_a - \hat\mu_b}{se(\hat\mu_a - \hat\mu_b)}\sim N(0,1)$$
Assumption:</p>
<ul>
<li>each trial is a Bernoulli</li>
<li>the number of groups are fixed</li>
<li>The underlying distribution is $y_a\sim binomial(n_a, \pi_a), y_b\sim binomial(n_b, \pi_b)$</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Contingency Table</strong>
test statistics $\chi^2 = \sum_j\sum_i (y_{ij} - \hat\mu_{ij})^2 / \hat\mu_{ij}\sim \chi^2_{(I-1)(J-1)}$ where $\hat\mu_{ij} = \pi_{i.}\pi_{.j}/n$</p>
<p>Contingency table model:<br>
$Y_{ij}$ be the r.v. representing the number of observations in the cell<br>
$y_{ij}$ be the observed cell counts</p>
<p>The underlying distribution of $Y=(Y_{11},...,Y_{nn})\sim Multinomial$
$$P(Y=y)=\frac{n!}{y_{11}!...y_{nn}!} \prod_{i,j}\pi_{ij}^{y_{ij}}$$</p>
<p>Using MLE subjecting to $\sum_{ij}\pi_{ij} = 1$, we get $\hat\mu_{ij} = y_{ij} / n$</p>
<p>With null hypothesis of independence we can get $\hat\mu_{ij} = \hat\mu_{i.}\hat\mu_{.j}$
Then we can use LRT where the full model contains the interaction terms
$$\log\mathcal L_F = \sum_{ij}y_{ij}\log(y_{ij}/n)$$
$$\log\mathcal L_R = \sum_{ij}y_{ij}\log(y_{i.}y_{.j}/nn)$$</p>
$$d.f. = (IJ-1)-(I+J-2)$$<p>
lose 1 for constraint $\sum_{ij}\pi_{ij} = 1$, lose 2 for constraints $\sum_i \pi_{i.}=1,\sum_j\pi_{.j}=1$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Fisher's-Exact-Test">Fisher's Exact Test<a class="anchor-link" href="#Fisher's-Exact-Test">&#182;</a></h4><ul>
<li>randomization test</li>
<li>appropriate for small sample size</li>
<li>assumes the row and column totals are fixed</li>
<li>p-value is calculated from hypergeometrix distribution
$$P=\frac{{a+b\choose a}{c+d\choose c}}{{n\choose a+c}}$$</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Poisson Regression</strong></p>
<ul>
<li>counts aren't fixed</li>
<li>treat IJ count as realizations of a Possion random variable</li>
</ul>
<p>Compare the interactions term</p>
<p>Three-way interactions</p>
<ul>
<li>complete independence: does not have any interaction terms</li>
<li>block independence: joint probability of two factors (say A,B) is independent of the third (C). Then include the interaction term between $AB$</li>
<li>partial independence: $P(AB\mid C)=P(A\mid C)P(B\mid C)$, AB are conditionally independent on $C$. Include interactions between $AC,BC$</li>
<li>Uniform association: include all two-way interactions</li>
<li>Saturated model: include three-way interactions</li>
</ul>

</div>
</div>
</div>
    </div>
  </div>
</section>
  <script src="https://lihd1003.github.io/assets/js/vendor.js"></script>
  <script src="https://lihd1003.github.io/assets/js/app.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script>
    $("table").addClass("table table-lined")
  </script>
</body>

 


</html>
