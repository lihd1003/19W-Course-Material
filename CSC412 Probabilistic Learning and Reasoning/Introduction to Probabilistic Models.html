<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />

<title>Introduction to Probabilistic Models - Notes Portal</title>

<link rel="icon" type="image/gif" href="https://lihd1003.github.io/assets/images/logo.png">
<link rel="stylesheet" href="https://lihd1003.github.io/assets/css/vendor.css" />
<link rel="stylesheet" href="https://lihd1003.github.io/assets/css/style.css" />
<link href="https://fonts.googleapis.com/css?family=Roboto&display=swap" rel="stylesheet">
<script src="https://kit.fontawesome.com/98fa07784c.js"></script>



<style type="text/css">
/* Overrides of notebook CSS for static HTML export */

div#notebook {
  overflow: visible;
  border-top: none;
}@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="https://lihd1003.github.io/UofT-Course-Material-Repo/index/custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
	<!-- header -->
   <header class="header-sticky header-dark">
    <div class="container">
      <nav class="navbar navbar-expand-lg navbar-dark">
        <a class="navbar-brand" href="./index.html">
          <img class="navbar-logo navbar-logo-light" src="https://lihd1003.github.io/assets/images/logo.png" alt="Logo">
          <img class="navbar-logo navbar-logo-dark" src="https://lihd1003.github.io/assets/images/logo.png" alt="Logo">
        </a>

        <div class="collapse navbar-collapse" id="navbarSupportedContent">
          <ul class="navbar-nav align-items-center mr-auto">
          </ul>

          <ul class="navbar-nav align-items-center mr-0">
            <li class="nav-item">
              <a href="https://github.com/lihd1003" class="nav-link waves-effect waves-light" target="_blank">
                <i class="icon-github mr-2"></i>GitHub
              </a>
            </li>
            <li class="nav-item">
              <a href="https://github.com/lihd1003/UofT-Course-Material-Repo" class="nav-link waves-effect waves-light" target="_blank">
                <i class="icon-star mr-2"></i>Star the Repo
              </a>
            </li>
          </ul>
        </div>
      </nav>
    </div>
  </header>

<section class="hero hero-with-header text-white" data-top-top="transform: translateY(0px);" data-top-bottom="transform: translateY(250px);">
    <div class="image image-overlay" style="background-image:url(https://lihd1003.github.io/assets/images/black.jpg)"></div>
    <div class="container">
      <div class="row align-items-center">
        <div class="col text-shadow">

          <h1 class="mb-0">Introduction to Probabilistic Models</h1>
        </div>
      </div>
    </div>
 </section>
 <section class="bg-white sec" id="project">
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Overview-of-Probabilistic-Models">Overview of Probabilistic Models<a class="anchor-link" href="#Overview-of-Probabilistic-Models">&#182;</a></h2><p>Let r.v. $\mathcal X = (X_1, ..., X_M)$ with the model relationships $p_\theta(\mathcal X)$.</p>
<p>Assuming a generative distribution, i.e. $(X_1,...,X_M) \sim p_{data}(\mathcal X)$</p>
<p>Then, the learning goal is to find best $p_{\theta^*} \approx p_{data}$. 
Therefore, the problem is to define "best" and methods to find the "best".</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="A-Probabilistic-Perspective-on-ML-Tasks">A Probabilistic Perspective on ML Tasks<a class="anchor-link" href="#A-Probabilistic-Perspective-on-ML-Tasks">&#182;</a></h2><h3 id="Supervised">Supervised<a class="anchor-link" href="#Supervised">&#182;</a></h3><p>Assuming the input data $X$, labels $C$, and continuous output $y$.</p>
<p>Given $\mathcal D = \{(x,c)_1,...,(x,c)_M\}$, assume $(x,c)\sim p_{data}(X,C)$<br>
<strong>classification</strong> Let $c$ be discrete $$p(C\mid X) = \frac{p(X,C)}{p(X)} = \frac{p(X,C)}{\sum_{c\in C} p(X,c)}$$</p>
<p><strong>regression</strong> Let $y$ be continuous 
$$p(Y\mid X) = \frac{p(X,Y)}{p(X)} = \frac{p(X,Y)}{\int_Y p(X,y)dy}$$</p>
<h3 id="Unsupervised">Unsupervised<a class="anchor-link" href="#Unsupervised">&#182;</a></h3><p>$\mathcal D = \{(x)_1,...,(x)_M\}$, assuming there exists some features (e.g. clustering) i.e. assuming $X, \_ \sim p(X,C_{hidden})$<br>
Then, we still want to transfer the question into $p_\theta (C\mid X)$</p>
<h3 id="Semi-supervised">Semi-supervised<a class="anchor-link" href="#Semi-supervised">&#182;</a></h3><p>When labels are partially observed, for example, matrix completion problems.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Latent-Variables">Latent Variables<a class="anchor-link" href="#Latent-Variables">&#182;</a></h3><p>Never observed, encode modeling assumptions, infer complex features</p>
<h3 id="Operations-on-the-probabilistic-models">Operations on the probabilistic models<a class="anchor-link" href="#Operations-on-the-probabilistic-models">&#182;</a></h3><p>Generating data $x\sim p_\theta(x)$<br>
Estimating likelihood<br>
Inferencing<br>
Learning the parameters</p>
<h3 id="Considerations-on-Prob.-Models">Considerations on Prob. Models<a class="anchor-link" href="#Considerations-on-Prob.-Models">&#182;</a></h3><p>efficient computations<br>
compact representations</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Dimensionality-of-Joint-Distribution">Dimensionality of Joint Distribution<a class="anchor-link" href="#Dimensionality-of-Joint-Distribution">&#182;</a></h3><p>Suppose we have variables $T=\{t_0,t_1\},W=\{w_0, w_1\},M = \{m_0, m_1\}$, then the joint distribution $p(T,W,M)$ is parameterized with $2\times 2 \times 2= 8$ parameters. Hence if we add more states onto each variables, say we have $k$ states for each of $n$ variables. Then, the join distribution will have $k^n$ parameters.</p>
<h4 id="Reduce-Dimensionality-of-Join-Distribution">Reduce Dimensionality of Join Distribution<a class="anchor-link" href="#Reduce-Dimensionality-of-Join-Distribution">&#182;</a></h4><p>With assumptions on the independence of variables, then we can reduce the dimensionality and reduce the number of parameters.</p>
<p>note that <strong>fully factorized</strong> means that 
$$p(T,W,M) = p(T)p(W\mid T)p(M\mid T,W) = p(M)p(T\mid M)p(W\mid T,M) =...$$</p>
<p>suppose we have the assumption that $T,W$ are independent, then the factorization</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Likelihood-Function">Likelihood Function<a class="anchor-link" href="#Likelihood-Function">&#182;</a></h2>$$l(\theta; x) = \log p(x\mid \theta)$$<p>For iid. $x$, 
$$l(\theta, x) = \log(\prod p(x^{(m)})\mid \theta) = \sum \log p(x^{(m)}\mid \theta)$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Sufficient-Statistics">Sufficient Statistics<a class="anchor-link" href="#Sufficient-Statistics">&#182;</a></h2><p>A <strong>statistic</strong> is a deterministic function of a set of r.v.</p>
<p>A <strong>sufficient statistic</strong> is a statistic that conveys exactly the same information about the generative distribution as the entire dataset itself. i.e., inferences made from sufficient statistic, $T(x)$, are the same as those obtained from the entire data. $T(X)$ is a sufficient statistic for $X$ if 
$$T(x^{(1)}) = T(x^{(2)})=L(\theta; x^{(1)}) = L(\theta; x^{(2)}), \forall \theta$$
a.k.a.
$$P(\theta| T(X)) = P(\theta|X)$$
By Neyman factorization theorem
$$p(\theta|T(X)) = f_\theta(X) = h(x, T(x))g(T(x), \theta)$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Example:-Sufficient-Statistics-for-Bernoulli">Example: Sufficient Statistics for Bernoulli<a class="anchor-link" href="#Example:-Sufficient-Statistics-for-Bernoulli">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Given a data set on $N$ observations $X \in \{0, 1\}^N $, then likelihood is 
\begin{align*}
l(\theta; D) &amp;= \log p(D|\theta) \\
&amp;= \log \sum_{i=1}^N \theta^{x_i}(1-\theta)^{1-x_i}\\
&amp;= \sum^N x_i \log\theta + (1-x_i)\log(1-\theta)\\
&amp;= \log\theta\sum^N x_i + \log(1-\theta)\sum^N (1-x_i)\\
&amp;= \log\theta N_H + \log(1-\theta)N_T\\
\end{align*}
so that $T(X) = N_H$ so that $h(x, T(x)) = 1, g(T(x), \theta) = \log\theta N_H + \log(1-\theta)(N-N_T)$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Example:-Sufficient-Statistic-for-Multinomial">Example: Sufficient Statistic for Multinomial<a class="anchor-link" href="#Example:-Sufficient-Statistic-for-Multinomial">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Given a data set $D \in \{1,2,..., K\}^N$ and the model is $p(x_n = i\mid \theta) = \theta_i$ w.r.t $\sum^K \theta_i = 1$
\begin{align*}
l(\theta; D) &amp;= \log \prod_n \theta_1^{x_n = 1}...\theta_K^{x_n=K}\\
&amp;= \sum_n\sum_i \mathbb I(x_n = i)\log\theta_i\\
&amp;= \sum_i \log\theta_i \sum_n \mathbb I(x_n = i)\\
&amp;= \sum_i \log\theta_i N_i
\end{align*}
So that $N_i$'s are sufficient statistics.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Sufficient-Statistics-for-general-Exponential-Family">Sufficient Statistics for general Exponential Family<a class="anchor-link" href="#Sufficient-Statistics-for-general-Exponential-Family">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In general, exponential family members have simple sufficient statistics for the natural statistics $\eta$
$$p(x|\eta) = h(x)\exp\{\eta^T T(x) - A(\eta)\}$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Sufficient-Statistics-for-Univariate-Normal">Sufficient Statistics for Univariate Normal<a class="anchor-link" href="#Sufficient-Statistics-for-Univariate-Normal">&#182;</a></h3>\begin{align*}
p(x|\theta) &amp;= \sqrt{2\pi\sigma^2}^{-1}\exp(-\frac{(x-\mu)^2}{2\sigma^2})\\
&amp;= \sqrt{2\pi\sigma^2}^{-1}\exp(-\frac{x^2}{2\sigma^2} + \frac{\mu x}{\sigma^2} - \frac{u^2}{\sigma^2})\\
&amp;= \sqrt{2\pi\sigma^2}^{-1}\exp(-\frac{x^2}{2\sigma^2} + \frac{\mu x}{\sigma^2})\exp( -\frac{u^2}{4\sigma^2})
\end{align*}<p>So that $T(x) = \begin{bmatrix}x^2\\x\end{bmatrix}, \eta = \begin{bmatrix}-1/2\sigma^2&amp; \frac{u}{\sigma^2}\end{bmatrix}$<br>
$h(x, T(x)) = \sqrt{2\pi}^{-1}, g(T(x), \eta) = \sigma^{-1}\exp(\eta^TT(x))\exp(\frac{\eta_1^2}{4\eta_2})$</p>

</div>
</div>
</div>
    </div>
  </div>
</section>
  <script src="https://lihd1003.github.io/assets/js/vendor.js"></script>
  <script src="https://lihd1003.github.io/assets/js/app.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script>
    $("table").addClass("table table-lined")
  </script>
</body>

 


</html>
