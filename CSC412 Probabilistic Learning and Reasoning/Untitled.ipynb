{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Settings and Goal\n",
    "For a density $p\\in\\mathcal P$, we want to give approximation $p(x) = \\frac{1}{z}\\tilde p(x)$ where $z$ is the normalizer that is intractable to compute and $\\tilde p$ is some function of $x$ that is not necessarily a probability. Then, to efficiently find an estimation, we choose some $q_\\phi\\in \\mathcal Q$ that $p$ may or may not in $\\mathcal Q$. \n",
    "\n",
    "The goal is to minimize the __loss__ between $p, q$. While the __loss__ may not be measured by Euclidean distance of the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullbackâ€“Leibler divergence\n",
    "\n",
    "Compare two distribution $p, q$, \n",
    "$$D_{KL}(p\\parallel q) = E_{x\\sim q}\\log \\frac{q(x)}{p(x)} = \\int \\log \\frac{q(x)}{p(x)} q(x)dx$$\n",
    "\n",
    "### Properties\n",
    "$D_{KL}(p\\parallel q) \\geq 0$  \n",
    "$D_{KL}(p\\parallel q) = 0 \\Leftrightarrow p=q$  \n",
    "$D_{KL}(p\\parallel q)\\neq D_{KL}(q\\parallel p)$ if $p\\neq q$\n",
    "\n",
    "Then, to assess $D_{KL}$  \n",
    "$p\\approx q\\Rightarrow D_{KL}$ small   \n",
    "$p$ large and $q$ small $\\Rightarrow$ ${D_{KL}(q\\parallel p)}$ small, ${D_{KL}(p\\parallel q)}$ large\n",
    "\n",
    "Then, if we find $q_{\\phi}$ that optimize $D_{KL}(q\\parallel p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$D_{KL}(q\\parallel p)$ is called __information projection__ since it is to capture the partial (but the majority) pattern of $p$.  \n",
    "Therefore, if we sample from such $q$, we might be biased for partial of the data. \n",
    "\n",
    "$D_{KL}(p\\parallel q)$ __moment projection__ since its objective is to estimate the moments.  \n",
    "Therefore, if we sample from such $q$, we may introduce new, non-existing features. \n",
    "\n",
    "Most times, we'd prefer to information projection since it does not introduce variances and captures the shape (although might be partially the shape). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evidence lower bound (ELBO)\n",
    "\n",
    "\\begin{align*}\n",
    "D_{KL}(q\\parallel p) &= E_{x\\sim q} \\log(\\frac{q(x)}{p(x)}) \\\\\n",
    "&= E_q \\log(q(x)) - \\log(\\frac{1}{z} + \\log(\\tilde p(x)))\\\\\n",
    "&= E_q [\\log q(x) - \\log(\\tilde p(x))] + \\log z\\\\\n",
    "&= D_{KL}(q\\parallel \\tilde p) + \\log z\n",
    "\\end{align*}\n",
    "Therefore, minimizing the information projection is the same as minimizing the unnormalized information projection. \n",
    "\n",
    "And that \n",
    "$$D_{KL}(q\\parallel \\tilde p) = D_{KL}(q\\parallel p) - \\log z \\geq -\\log z \\geq - \\log p(\\mathcal D)$$\n",
    "hence it is a bound on the NLL of the data. Let __Evidence lower bound (ELBO)__ $=-D_{KL}(q\\parallel \\tilde p) \\leq \\log(p(\\mathcal D))$ so that we turn intractable integration into an optimization problem. "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
